# Zookeeper

zookeeper基于ZAB协议实现：
所有的事务请求通过唯一的Leader服务器处理，其余的被称为Follower。Leader接收客户端的一个请求，并且将其转换为一个Proposal（提议），然后将该Proposal广播给所有的Follower，如果接收到半数以上的Follower的正确反馈，那么会发送Commit命令给所有的Follower，提交该Proposal。

ZAB协议分为下面三个阶段：

发现（Discover阶段）：即选举Leader过程。
同步阶段：即选举完成后，Follower或者Observer同步最新Leader的最新数据。
广播阶段：当同步完成之后，接收客户端的请求，广播给所有的服务器，实现数据在集群中的多副本存储。

三种角色：

Leader：处理事务请求。
Follower：处理非事务请求，转发事务请求给Leader，参与Leader的选举和Proposal的提议。
Observer：处理非事务请求，转发事务请求给Leader，不参与选举，主要为了提高读性能。

zookeeper中server有四种状态

1.LOOKING：竞选状态

2.OBSERVING：观察

3.FOLLOWING：跟随者

4.LEADER：领导者

## 选举

Leader选举有两种情况：一是服务器初始化启动。二是Leader宕机。

1. 服务器启动时期的Leader选举

**(1) 每个Server发出一个投票。每次投票会包含myid和Zxid**（myid：serverid，zxid：事务id）**，第一次投票都投自己。**

**(2) 接受来自各个服务器的投票。并判断有效性，如检查是否是本轮投票、是否来自LOOKING状态的服务器。**

**(3) 处理投票。针对每一个投票，优先比较ZXID，其次myid，数值大的为Leader。然后再次向其他机器发送投票。**

**(4) 统计投票。每次投票后，服务器都会统计投票信息，判断是否有过半机器接受到相同的投票信息，有的话就认为已经选出了Leader。**

**(5）确定Leader后，Follower服务器改变状态为FOLLOWING，Leader改变为LEADING。**

\2. 服务器运行时期的Leader选举

在Zookeeper运行期间，Leader与非Leader服务器各司其职，即便当有非Leader服务器宕机或新加入，此时也不会影响Leader，但是一旦Leader服务器挂了，那么整个集群将暂停对外服务，进入新一轮Leader选举，其过程和启动时期的Leader选举过程基本一致。假设正在运行的有Server1、Server2、Server3三台服务器，当前Leader是Server2，若某一时刻Leader挂了，此时便开始Leader选举。选举过程如下

(1) **变更状态**。Leader挂后，余下的非Observer服务器都会讲自己的服务器状态变更为LOOKING，然后开始进入Leader选举过程。

(2) **每个Server会发出一个投票**。在运行期间，每个服务器上的ZXID可能不同，此时假定Server1的ZXID为123，Server3的ZXID为122；在第一轮投票中，Server1和Server3都会投自己，产生投票(1, 123)，(3, 122)，然后各自将投票发送给集群中所有机器。

(3) **接收来自各个服务器的投票**。与启动时过程相同。

(4) **处理投票**。与启动时过程相同，此时，Server1将会成为Leader。

(5) **统计投票**。与启动时过程相同。

(6) **改变服务器的状态**。与启动时过程相同。

链接：https://www.jianshu.com/p/c2ced54736aa

# ISTIO

**Istio 是一个Service mesh架构，虽说谷歌说他是独立于平台的，但实际都是和Kubernetes 一起用的。**

**分为数据面和控制面，数据面把一个代理通过sidecar形式注入Pod，然后通过修改IPtables进行流量劫持。创POD是会创建 一个pause 容器生成 network namespace，sidecar就是替换了这个容器，从而共享namespace**

**控制面有Pilot，MIXER，Citadel几个组件。Plilot做的是对接K8s的Etcd（K8s用它来服务注册），分发配置。MIXER负责控制策略，每个请求sidecar先发给MIXER，MIXER进行决策。Citadel就是用来认证的。**

- 服务发现：探测所有可用的上游或后端服务实例
- 流量路由：将网络请求路由到正确的上游或后端服务
- 负载均衡：在对上游或后端服务进行请求时，选择合适的服务实例接收请求，同时负责处理超时、断路、重试等情况
- 身份验证和授权：对网络请求进行身份验证、权限验证，以决定是否响应以及如何响应，使用 mTLS 或其他机制对链路进行加密等
- 链路追踪：对于每个请求，生成详细的统计信息、日志记录和分布式追踪数据，以便操作人员能够理解调用路径并在出现问题时进行调试

# 分布式

## 分布式锁

不同服务器竞争资源要用分布式锁。有三种实现方式 mysql乐观锁，redis，zookeeper。

**redis使用setnx+expire指令。setnx只会对不存在的key设值，如果Key存在，获取锁失败。**

**比较**

redis锁需要自己不断尝试获取锁，消耗CPU大。客户端挂了只能等锁超时。

expire时间要大于线程所需时间，否则会出现第一个线程的锁过期，删掉了第二个线程的锁。

redis分布式锁的问题：

1 setnx 和expire非原子操作，setnx成功，expire失败

解决方案：set（lock_sale_商品ID，1，30，NX）

2  线程A在expire时间内未执行完业务逻辑，锁超时自动释放，此时B获得了锁，A在退出时删除了B的锁。

 加锁的时候把当前的线程 ID 当做 `value`，并在删除之前验证 `key` 对应的 `value` 是不是自己线程的 ID。 

3 线程A在expire时间内未执行完业务逻辑，锁超时自动释放，此时B获得了锁，两个线程同时占用资源。

开启一个守护进程，给快要过期的锁续期。执行完任务时，显式关闭守护进程。





**Zookeeper 的有序节点**

**分布式锁实现**

- **创建一个锁目录 /lock；**
- **当客户端需要获取锁时，在 /lock 下创建有序的临时子节点；如果是序号最小的子节点，则获得锁；否则监听自己的前一个子节点，获得子节点的变更通知后重复步骤直至获得锁；**
- **执行业务代码，完成后，删除对应的子节点。**

1. Zookeeper 抽象模型

Zookeeper 提供了一种树形结构的命名空间。zookeeper=文件系统+监听通知机制

[![img](https://camo.githubusercontent.com/50116cf32931af95e1180b7fbd4193db0a4da49c/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f61656661383034322d313566612d346538622d396635302d3230623238326132633632342e706e67)](https://camo.githubusercontent.com/50116cf32931af95e1180b7fbd4193db0a4da49c/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f61656661383034322d313566612d346538622d396635302d3230623238326132633632342e706e67)



2. 节点类型

- 永久节点：不会因为会话结束或者超时而消失；
- 临时节点：如果会话结束或者超时就会消失；
- 有序节点：会在节点名的后面加一个数字后缀，并且是有序的，例如生成的有序节点为 /lock/node-0000000000，它的下一个有序节点则为 /lock/node-0000000001，以此类推。

3. 监听器

为一个节点注册监听器，在节点状态发生改变时，会给客户端发送消息。

4. **分布式锁实现**

- **创建一个锁目录 /lock；**
- **当一个客户端需要获取锁时，在 /lock 下创建临时的且有序的子节点；**
- **客户端获取 /lock 下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁；否则监听自己的前一个子节点，获得子节点的变更通知后重复此步骤直至获得锁；**
- **执行业务代码，完成后，删除对应的子节点。**

5. 会话超时

如果一个已经获得锁的会话超时了，因为创建的是临时节点，所以该会话对应的临时节点会被删除，其它会话就可以获得锁了。可以看到，这种实现方式不会出现数据库的唯一索引实现方式释放锁失败的问题。

6. 羊群效应

一个节点未获得锁，只需要监听自己的前一个子节点，这是因为如果监听所有的子节点，那么任意一个子节点状态改变，其它所有子节点都会收到通知（羊群效应，一只羊动起来，其它羊也会一哄而上），而我们只希望它的后一个子节点收到通知。



## CAP

分布式系统不可能同时满足一致性（C：Consistency）、可用性（A：Availability）和分区容忍性（P：Partition Tolerance），最多只能同时满足其中两项。

**一致性**

**一致性指的是多个数据副本保持一致性，也就是说读操作要返回最新的写操作**，在一致性的条件下，系统在执行数据更新操作之后能够从一致性状态转移到另一个一致性状态。

对系统的一个数据更新成功之后，如果所有用户都能够读取到最新的值，该系统就被认为具有强一致性。

**可用性**

可用性指分布式系统在面对各种异常时可以提供正常服务的能力，可以用系统可用时间占总时间的比值来衡量，4 个 9 的可用性表示系统 99.99% 的时间是可用的。

**可用性要求非故障的节点在合理的时间内返回合理的响应**，对于用户的每一个操作请求总是能够在有限的时间内返回结果。

**分区容错性**

节点被划分为多个区域，区域之间无法通信时，系统能够正常工作。

**权衡**

CAP只能三选二，因为需要假设网络不可靠，所以分区容错性必须满足。

只能在A和P之间做选择。就有了BASE理论

对于CP来说，放弃可用性，追求一致性和分区容错性，我们的zookeeper其实就是追求的强一致。

对于AP来说，放弃一致性(这里说的一致性是强一致性)，追求分区容错性和可用性，这是很多分布式系统设计时的选择，后面的BASE也是根据AP来扩展。

## BASE

BASE 是基本可用（Basically Available）、软状态（Soft State）和最终一致性（Eventually Consistent）三个短语的缩写。 

**BASE 理论是对 CAP 中一致性和可用性权衡的结果，通过牺牲强一致性达到可用性，只要求最终一致性。**

**基本可用**

指分布式系统在出现故障的时候，保证核心可用，允许损失部分可用性。

例如，电商在做促销时，为了保证购物系统的稳定性，部分消费者可能会被引导到一个降级的页面。

**软状态**

指允许系统中的数据存在中间状态，进行同步的过程可以存在时延。

**最终一致性**

最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能达到一致的状态。

ACID 要求强一致性，通常运用在传统的数据库系统上。而 BASE 要求最终一致性，通过牺牲强一致性来达到可用性，通常运用在大型分布式系统中。

在实际的分布式场景中，不同业务单元和组件对一致性的要求是不同的，因此 ACID 和 BASE 往往会结合在一起使用。

## 分布式事务

解决问题：分布式环境下， 微服务过多时，事务跨越多个节点时，每个节点提交事务时无法知道其他节点成功还是失败。

一个server减库存事务成功，另一个下订单事务失败。

![1585059035167](C:\Users\HASEE\AppData\Roaming\Typora\typora-user-images\1585059035167.png)

### 二阶段提交

**两阶段提交，用一个协调者来协调参与者的行为，并决定是否执行事务。**

**准备阶段：协调者给参与者发送 Prepare 消息，参与者返回失败(如权限验证失败)，或者执行事务，写本地的 redo 和 undo 日志，但不提交。**

**提交阶段：如果协调者收到失败消息或者超时，直接给参与者发送回滚(Rollback)消息；否则，发送提交(Commit)消息；参与者根据指令执行提交或者回滚操作，释放所有事务处理过程中使用的锁资源。**

**存在的问题**：

**同步阻塞**

事务参与者在等待时都处于同步阻塞等待状态。协调者如果挂掉，参与者会一直同步阻塞等待，无法完成其它操作。

**数据不一致**

在提交阶段，如果协调者只发送了部分 Commit 消息，此时网络发生异常，那么只有部分参与者接收到 Commit 消息，也就是说只有部分参与者提交了事务，使得系统数据不一致。

**太过保守**

任意一个节点失败就会导致整个事务失败。

### 三阶段提交

三阶段提交改进了两阶段提交。 

3PC 把 2PC 的准备阶段再次一分为二，有 CanCommit、PreCommit、DoCommit 三个阶段。

在参与者中也引入超时机制，如果CanCommit和PreCommit阶段协调者超时，则执行abort，如果在DoCommit阶段超时，参与者执行commit。

 ![img](https://upload-images.jianshu.io/upload_images/6468203-2f3326f629d909e9.png?imageMogr2/auto-orient/strip|imageView2/2/w/611/format/webp) 

**CanCommit** **阶段** 

协调者向参与者发送 commit 请求，参与者如果可以提交就返回 Yes 响应，否则返回 No 响应。 

**PreCommit** **阶段** 

协调者根据参与者的响应情况来决定是否可以继续进行。

 如果都是Yes 响应， 协调者就向参与者发送PreCommit请求，参与者执行事务，写本地的 redo 和 undo 日志
，返回ACK响应。 

如果存在No 响应，或者等待超时，协调者就发送abort请求。参与者收到abort指令或者等待超时后执行事务中断，返回ACK。

**doCommit** **阶段** 

协调者根据相应发送docommit或者abort指令。

参与者收到指令后提交事务或者利用undo日志回滚事务，并释放资源。

参与者向协调者发送ack指令。

协调者收到所有ack后，完成事务或者中断事务。

**三阶段提交协议的缺点**

如果进入PreCommit后，协调者发出的是abort请求，假设只有一个参与者收到并进行了abort操作，
而其他对于系统状态未知的参与者超时后会继续Commit，此时系统状态发生不一致性。

### 补偿事务（TCC）

**TCC 用补偿机制，针对每个操作，都要注册一个Confirm 和Cancel 操作。它分为三个阶段：**

- **Try 阶段完成业务检查（一致性），预留业务资源(准隔离性)**
- **Confirm 阶段使用Try阶段预留资源，确认执行业务操作，Try成功，Confirm一定成功。**
- **Cancel 阶段是预留资源失败，释放Try阶段预留资源。**

 ![img](https://user-gold-cdn.xitu.io/2018/7/26/164d74a2293772d5?imageView2/0/w/1280/h/960/format/webp/ignore-error/1) 

举个例子，假入 Bob 要向 Smith 转账，思路大概是：
 我们有一个本地方法，里面依次调用
 1、首先在 Try 阶段，要先调用远程接口把 Smith 和 Bob 的钱给冻结起来。
 2、在 Confirm 阶段，执行远程调用的转账的操作，转账成功进行解冻。
 3、如果第2步执行成功，那么转账成功，如果第二步执行失败，则调用远程冻结接口对应的解冻方法 (Cancel)。

**优点：** 两阶段提交一直持有锁，实现强一致性，TCC不会一直持有锁，实现最终一致性

 1.解决了协调者单点，由主业务方发起并完成这个业务活动。业务活动管理器也变成多点，引入集群。 2不会锁定整个资源，将资源转换为业务逻辑形式，粒度变小。 3.有了补偿机制之后，实现最终一致性。

https://blog.csdn.net/u013380694/article/details/83347764

###  异步消息确保型 

将一系列同步的事务操作变为基于消息执行的异步操作, 避免了分布式事务中的同步阻塞操作的影响。

![1585111458312](C:\Users\HASEE\AppData\Roaming\Typora\typora-user-images\1585111458312.png)

 https://juejin.im/post/5b5a0bf9f265da0f6523913b#heading-6 

### 本地消息表（待补充）

 https://www.cnblgs.com/FlyAway2013/p/10124283.html 



# 一致性算法

## Paxos

用对多个节点值，保证选出唯一值。

主要有三类节点：

- 提议者（Proposer）：提议一个值；
- 接受者（Acceptor）：投票；
- 告知者（Learner）：被告知投票的结果，不参与投票过程。

**执行过程**

**提议包含：[n, v]， n 为序号（具有唯一性），v 为提议值。执行过程有三个阶段**

**Prepare 阶段，每个提议者向所有接受者 发送 Prepare 请求。接受者必须接受收到的第一个 Prepare 请求，借来下每次接收的序号都要大于上一次，每次接受发出一个Prepare 响应**

 **Accept 阶段，当一个提议者接收到超过一半接受者 的 Prepare 响应时，就发送 Accept 请求。**

**Learn 阶段，接受者接收到 Accept 请求时，如果序号大于要求的序号，那么就发送 Learn 提议给所有的 告知者。当告知者发现有大多数的 接受者接收了某个提议，那么该提议的提议值就被 Paxos 选择出来。**

[![img](https://camo.githubusercontent.com/9f55215762db7fd8dabf8001891f1fcb62d3260b/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f62663636373539342d626234622d343633342d626639622d3035393661343534313562612e6a7067)](https://camo.githubusercontent.com/9f55215762db7fd8dabf8001891f1fcb62d3260b/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f62663636373539342d626234622d343633342d626639622d3035393661343534313562612e6a7067)



**约束条件**

**1. 正确性**

指只有一个提议值会生效。

因为 Paxos 协议要求每个生效的提议被多数 Acceptor 接收，并且 Acceptor 不会接受两个不同的提议，因此可以保证正确性。

**2. 可终止性**

指最后总会有一个提议生效。

Paxos 协议能够让 Proposer 发送的提议朝着能被大多数 Acceptor 接受的那个提议靠拢，因此能够保证可终止性。

## RAFT

**由于paxos算法过于复杂，产生了更易懂的raft算法。**

**Raft协议中，节点有三个状态：Leader、Follower和Candidate。Raft选举由Candidate主动发起选举leader。**

**Leader接受写和读请求，周期性向follower发送 AppendEntries。Candidate发起选举。如果一个Follower一个Term内没有收到AppendEntries，则它转为Candidate，并再随机150-300ms后发起选举，如果获得过半节点的投票，就会成为leader。**

采取随机时间的目的是避免多个Followers同时发起选举，导致选举无效需重选。

**Term**

在 Raft 中每一轮选举都是一个 Term 周期，在一个 Term 中只能产生一 个 Leader；当某节点收到的请求中 Term 比当前 Term 小时则拒绝该请求。

**选举**

Raft 的选举由定时器来触发，每个节点的选举定时器时间都是不一样的，开始时状态都为 Follower 某个节点定时器触发选举后 Term 递增，状态由 Follower 转为 Candidate，向其他节点 

发起 RequestVote RPC 请求，这时候有三种可能的情况发生： 

 1：该 RequestVote 请求接收到 n/2+1（过半数）个节点的投票，从 Candidate 转为 Leader， 

向其他节点发送 heartBeat 以保持 Leader 的正常运转。 

 2：在此期间如果收到其他节点发送过来的 AppendEntries RPC 请求，如该节点的 Term 大 

则当前节点转为 Follower，否则保持 Candidate 拒绝该请求。 

 3：Election timeout 发生则 Term 递增，重新发起选举 

 在一个 Term 期间每个节点只能投票一次，所以当有多个 Candidate 存在时就会出现每个 Candidate 发起的选举都存在接收到的投票数都不过半的问题，这时每个 Candidate 都将 Term 递增、重启定时器并重新发起选举，由于每个节点中定时器的时间都是随机的，所以就不会多次 存在有多个 Candidate 同时发起投票的问题。 

 在 Raft 中当接收到客户端的日志（事务请求）后先把该日志追加到本地的 Log 中，然后通过 heartbeat 把该 Entry 同步给其他 Follower，Follower 接收到日志后记录日志然后向 Leader 发送 ACK，当 Leader 收到大多数（n/2+1）Follower 的 ACK 信息后将该日志设置为已提交并追加到 本地磁盘中，通知客户端并在下个 heartbeat 中 Leader 将通知所有的 Follower 将该日志存储在 自己的本地磁盘中。 

# BIO/NIO

**BIO是阻塞IO**(必须等到数据接收完毕才可执行业务逻辑)，**用流传输数据**(InputStream/OutputStream)
**NIO是非阻塞的，通过IO多路复用实现，以块的方式处理数据。他在直接内存中分配一个buffer缓冲区读写**

**AIO是异步IO，内核准备好数据后调回调方法。**


缓冲区（Buffer）、通道（Channel）、选择器（Selector）

通道 Channel 是对原 I/O 包中的流的模拟，是双向的，可以同时用于读写。

通道包括以下类型：

- FileChannel：从文件中读写数据；
- DatagramChannel：通过 UDP 读写网络中数据；
- SocketChannel：通过 TCP 读写网络中数据；
- ServerSocketChannel：可以监听新进来的 TCP 连接，对每一个新进来的连接都会创建一个 SocketChannel。

**缓冲区**

缓冲区实质上是一个数组，NIO不会直接对通道进行读写数据，而是要先经过缓冲区。

**selector选择器**

底层是IO 多路复用中的 Reactor 模型，把channel注册到selector，一个thread使用一个Selector用轮询方式监听多个channel上的事件。

```java
while (true) {
    int num = selector.select();
    Set<SelectionKey> keys = selector.selectedKeys();
    Iterator<SelectionKey> keyIterator = keys.iterator();
    while (keyIterator.hasNext()) {
        SelectionKey key = keyIterator.next();
        if (key.isAcceptable()) {
            // ...
        } else if (key.isReadable()) {
            // ...
        }
        keyIterator.remove();
    }
}
```



注意：

套接字 Channel 才能配置为非阻塞，而 FileChannel 不能，为 FileChannel 配置非阻塞也没有意义。

# IO多路复用

**介绍：**

**IO有两阶段，第一阶段是等待socket数据到达内核，第二阶段是把数据复制到进程中。**

**IO多路复用第一阶段用一个线程通过select，poll，epoll轮询所有socket，当某个socket有数据到达了，就通知用户进程，第二阶段用户 recvfrom 把数据从内核复制到进程中。** 

**三函数：**

​	**select是用32个32位整数标识标识文件描述符，最多1024个，每次都要遍历才能找到就绪的描述符**
​     **poll使用链表保存文件描述符，没有数量限制，还是需要遍历。**
​     **epoll用红黑树存贮文件描述符，把就绪的事件放入一个链表中，不用遍历，只要判断链表是否位空**

其中epoll是Linux所特有，而select则一般操作系统均有实现。

**epoll**

由三个系统调用组成。epoll_create是初始化，创建红黑树，epoll_ctl是把就绪事件放入链表。epoll_wait判断链表是否位空。

## 水平触发/边缘触发

- 水平触发（LT，Level Trigger）模式下，文件描述符就绪时，每次调用 epoll_wait()都会触发通知；
- 边缘触发（ET，Edge Trigger）模式下，当描述符从未就绪变为就绪时通知一次，之后不会再通知，直到再次从未就绪变为就绪（缓冲区从不可读/写变为可读/写）。
- 区别： 边缘触发效率更高，减少了被重复触发的次数，函数不会返回大量用户程序可能不需要的文件描述符。
- 为什么边缘触发一定要用非阻塞（non-block）IO：避免由于一个描述符的阻塞读/阻塞写操作让处理其它描述符的任务出现饥饿状态。

## 零拷贝

# 零拷贝

零拷贝是指避免在用户态(User-space) 与内核态(Kernel-space) 之间来回拷贝数据的技术。

## 传统IO

传统IO读取数据并通过网络发送的流程，如下图

![640?wx_fmt=gif](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9PcVRBbDNXVEM3RWpuMG9YV3NiNEpaVE9nT0g0MEY1bUpYWUdiNWliaWJ1YWZpYkVnTEZDaWJ5RXRVbTVMaWN3MEhKbmJEZDU3MUNBQWZqZ3U5UXZ4OUROVmpnLzY0MD93eF9mbXQ9Z2lm)

1. read()调用导致上下文从用户态切换到内核态。内核通过sys_read()（或等价的方法）从文件读取数据。DMA引擎执行第一次拷贝：从文件读取数据并存储到内核空间的缓冲区。
2. 请求的数据从内核的读缓冲区拷贝到用户缓冲区，然后read()方法返回。read()方法返回导致上下文从内核态切换到用户态。现在待读取的数据已经存储在用户空间内的缓冲区。至此，完成了一次IO的读取过程。
3. send()调用导致上下文从用户态切换到内核态。第三次拷贝数据从用户空间重新拷贝到内核空间缓冲区。但是，这一次，数据被写入一个不同的缓冲区，一个与目标套接字相关联的缓冲区。
4. send()系统调用返回导致第四次上下文切换。当DMA引擎将数据从内核缓冲区传输到协议引擎缓冲区时，第四次拷贝是独立且异步的。

> 内存缓冲数据(上图中的read buffer和socket buffer )，主要是为了提高性能，内核可以预读部分数据，当所需数据小于内存缓冲区大小时，将极大的提高性能。

![640?wx_fmt=gif](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9PcVRBbDNXVEM3RWpuMG9YV3NiNEpaVE9nT0g0MEY1bXA2a3VNOTQyQ2JMY0J5MmhQdVkwZ21LY0NYamJSanhrVVlQb0NSTE5ObGxxZ0R1SXVUVVRUdy82NDA_d3hfZm10PWdpZg)

磁盘到内核空间属于DMA拷贝，用户空间与内核空间之间的数据传输并没有类似DMA这种可以不需要CPU参与的传输方式，因此用户空间与内核空间之间的数据传输是需要CPU全程参与的（如上图所示）。

> DMA拷贝即直接内存存取，原理是外部设备不通过CPU而直接与系统内存交换数据

所以也就有了使用零拷贝技术，避免不必要的CPU数据拷贝过程。

## NIO的零拷贝

NIO的零拷贝由transferTo方法实现。transferTo方法将数据从FileChannel对象传送到可写的字节通道（如Socket Channel等）。在transferTo方法内部实现中，由native方法transferTo0来实现，它依赖底层操作系统的支持。在UNIX和Linux系统中，调用这个方法会引起sendfile()系统调用，实现了数据直接从内核的读缓冲区传输到套接字缓冲区，避免了用户态(User-space) 与内核态(Kernel-space) 之间的数据拷贝。

![640?wx_fmt=gif](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9PcVRBbDNXVEM3RWpuMG9YV3NiNEpaVE9nT0g0MEY1bURvY2hpYXlsUjV3OWZpYUZMWTNUQmN4aWJQMzZYcDRSS1J5eVh1WVdsRWdwdWlicGtJdGNVaWE0RWlhZy82NDA_d3hfZm10PWdpZg)

使用NIO零拷贝，流程简化为两步：

1. transferTo方法调用触发DMA引擎将文件上下文信息拷贝到内核读缓冲区，接着内核将数据从内核缓冲区拷贝到与套接字相关联的缓冲区。
2. DMA引擎将数据从内核套接字缓冲区传输到协议引擎（第三次数据拷贝）。

内核态与用户态切换如下图：

![640?wx_fmt=gif](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9PcVRBbDNXVEM3RWpuMG9YV3NiNEpaVE9nT0g0MEY1bWZNamljZWVqQ3piSnYwaDJtU2xlWTBNV1RPS3F6NWlhVVdzYW02WVN2V1JwUnVrTzVTQUdrVG9nLzY0MD93eF9mbXQ9Z2lm)

相比传统IO，使用NIO零拷贝后改进的地方：

1. 我们已经将上下文切换次数从4次减少到了2次；
2. 将数据拷贝次数从4次减少到了3次（其中只有1次涉及了CPU，另外2次是DMA直接存取）。

如果底层NIC（网络接口卡）支持gather操作，可以进一步减少内核中的数据拷贝。在Linux 2.4以及更高版本的内核中，socket缓冲区描述符已被修改用来适应这个需求。这种方式不但减少上下文切换，同时消除了需要CPU参与的重复的数据拷贝。

![640?wx_fmt=gif](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9PcVRBbDNXVEM3RWpuMG9YV3NiNEpaVE9nT0g0MEY1bVMwV0s1N3BRQlVJcXhSV0gyMFF4RlNoenhWVW9PVldacVNaemRpYmQzVnhEaWJBZWVncnhVc3NnLzY0MD93eF9mbXQ9Z2lm)

用户这边的使用方式不变，依旧通过transferTo方法，但是方法的内部实现发生了变化：

1. transferTo方法调用触发DMA引擎将文件上下文信息拷贝到内核缓冲区。
2. 数据不会被拷贝到套接字缓冲区，只有数据的描述符（包括数据位置和长度）被拷贝到套接字缓冲区。DMA 引擎直接将数据从内核缓冲区拷贝到协议引擎，这样减少了最后一次需要消耗CPU的拷贝操作。

NIO零拷贝适用于以下场景：

1. 文件较大，读写较慢，追求速度
2. JVM内存不足，不能加载太大数据
3. 内存带宽不够，即存在其他程序或线程存在大量的IO操作，导致带宽本来就小

# UML图

**用例图**

参与者作为外部用户与系统交互。

用例是外部可见的系统功能单元。

关联表示参与者与用例之间的通信，任何一方都可发送或接受消息。箭头指向消息接收方。

泛化就是继承关系，箭头指向父用例。

包含把一个复杂用例表示的功能分解成较小的步骤。箭头指向分解出来的功能用例

扩展为基础用例提供一个附加功能。指向基础用例

**类图**

表示类、接口和它们之间的协作关系。

继承：带三角箭头的实线

实现：带三角箭头的虚线

聚合是整体与部分的关系
【代码体现】：成员变量
【箭头及指向】：带空心菱形的实心线，菱形指向整体

组合：带实心菱形的实线，菱形指向整体

依赖：带箭头的虚线，指向被使用者

![这里写图片描述](https://img-blog.csdn.net/20180621190026671?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1NDk1NzYz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

**时序图**

时序图描述对象的时间顺序。纵轴为时间，横轴为对象。对象存在时用一条虚线表示，生命线是一个双道线。消息用从一个对象的生命线到另一个对象生命线的箭头表示。箭头以时间顺序在图中从上到下排列。如下图：

四个元素（对象，生命线，消息，激活）
（1）对象就是类的实例化。
（2）生命线表示对象的存在时间
（3）消息是对象之间的单路通信。
消息的类型大致有：同步与异步，返回、阻止和超时。
（4）激活
表示这个时间，对象实现操作。

# git

Git 是分布式版本控制系统，github作为中心服务器用来交换每个用户的修改。

新建一个目录后，当前目录成为工作区，工作区有个隐藏目录.git是版本库，其中有一个stage（或者叫index）的暂存区，还有第一个分支master，以及指向master的一个指针HEAD。

**添加文件时，分两步执行**

第一步是用“git add”把文件添加到暂存区；
第二步 是用“git commit”提交更改，实际上就是把暂存区的所有内容提交到当前分支。

**分支**

分支实现：使用指针将每个提交连接成一条时间线，HEAD 指针指向当前分支指针。

```console
$ git branch testing
```

这会在当前所在的提交对象上创建一个指针。

**分支合并**：git merge testing

git push将本地分支合并到远程分支上。git pull 将远程分支合并到本地分支。 

**分支冲突**：Git用`<<<<<<<`，`=======`，`>>>>>>>`标记出不同分支的内容，修改冲突，然后git add，commit，push