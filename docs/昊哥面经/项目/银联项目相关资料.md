# 项目背景知识

## OpenStack

Openstack 是一个开源的云计算管理平台.

### Neutron 

Neutron 为 Open Stack 提供了基于SDN的网络服务,Neutron 管理的网络资源对象包括以下几个：  
	网络（network）：是一个位于 Layer2 的网络广播域，每个网络会有一个所属的用户，一个用户也可拥有多个网络对象。在一个网络中，可以包含数个子网和端口。一个网络如果需要正常与内部网络或外部网络通信，需要创建路由，路由实现了子网之间，或者实现子网同外部网络之间的连接。 

​	子网（Subnet）：是一个位于 Layer3 的虚拟网络，同一个租户下的子网需要有不同的 IP 地址段。如果没有路由的连接，只有连接到同一个子网的虚拟机实例彼此之间 Layer3 可见。如果不同的子网之间的实例互相访问，需要通过创建路由设备，连接两个子网，才能建立网络通信链接。 

​	端口（Port）：拥有属于自己的 MAC 地址，是一种软件实现的虚拟交换端口。端口一端和一个子网绑定，另一端与其他子网，虚拟机，容器绑定。所有的对象都需要通过端口接入某个子网。 

​	只需要运行 Neutron 的agent 进程，基于 Linuxbridge 或是 OpenVSwitch 建立基础网络，就可以为虚拟
机，容器等实例提供跨主机访问，访问外网，以及租户隔离等功能。 

### Octavia

Octavia 是 Open Stack 中的负载均衡组件，设计理念是 LBaaS(Load Balancing as a Service)，即为 Open Stack 平台提供负载均衡服务。其中基本对象概念如下： 

​	Load Balancer：负载均衡服务的基础对象，用户对负载均衡的配置和操作都基于此，具体承载对象是一个虚拟机实例。 

​	VIP：与向外提供负载均衡服务的虚拟 IP 地址，每个 LB 实例至少有一个 VIP，外部对后端业务的访问全部由此进入。 

​	Listener：从属于 LB 的监听器，用户可配置监听外部对 VIP 的访问类型，如：监听的端口，协议等。 

​	Pool：负载均衡后端的业务实例集群，用户会根据后端的业务类型进行划分，一个负载均衡实例下可以有多个 Pool。 

​	Member：Pool 中的成员，是承载后端业务的具体实例。 

​	Health Monitor：挂靠于成员池，顾名思义，健康监控器会周期性检查成员池中成员的健康状况，这里的健康是指网络可达。 

​	L7 Policy：七层网络转发策略，描述了对于指定数据包的转发规则，如：转发至某个 Pool，转发至某个 URL，或是禁止转发。 Octavia 的实际负载均衡业务模块与 Nova 配合，负责负载均衡实例的生命周期管理，与Neutron 配合，负责负载均衡网络的接入。健康检查模块会对后端成员的健康状况进行监控，如果出现网络不可达等故障状况，会自动进行故障转移作业。所有负载均衡的网络，实例，成员等配置信息均存储在数据库之中，会有特定的进程负责定期检查和管理。 

基本概念之间的交互流程如下图：

![img](https://img2018.cnblogs.com/blog/1628496/201907/1628496-20190718210436501-155725218.png)

在 octavia 中，资源之间的映射关系如下：

lb： 虚拟机/容器
listener： 虚拟机里面的一个 haproxy 进程

pool： haproxy 配置中的一个 backend

member： backend 配置中的一个 member

##  Kubernetes 

​	Kubernetes 是谷歌公司于 2015 年推出的一个大型容器集群编排管理工具。

​	在 Kubernetes 中，容器以一种 POD 的形式存在，POD 是一组或一个容器的集合，系统以 POD 为单位，提供启动，调度，更新，停止，删除等操作，并在此基础之上建立一整套完整的容器编排体系。

​	容器组的一个特点是无状态，重启容器之后 IP 地址可以发生变化，如果想要保证能够固定对外提供服务的访问地址，可以通过 Service 对象发布服务，前端访问可以通过虚拟的 Service IP 与可靠的与后端容器通信。 

### 组件

**Master**

1.Api Server:为集群的核心组件，主要负责集群各功能模块之间的通信，集群内的功能模块通过Api server将信息存入到分布式文件系统etcd中与其他节点进行信息交互。
2.Scheduler:负责集群的资源调度,跟踪集群中所有Node的资源利用情况,对新创建的pod，采取合适的调度策略，进行均衡的调度到合适的节点上。
3.Controller Manager:主要负责集群的故障检测和恢复的自动化，它内部的组件如下:
a.endpointController:定期关联service和pod，关联信息由endpoint负责创建和更新。
b.ReplicationController:完成pod的复制或移除，以确保ReplicationController定义的复本数量与实际运行pod的数量一致性

**Node：**是集群中的工作主机，Node可以是物理主机，也可以是虚拟机，包含以下组件:
1.kubelet:运行在每个Node节点上,kubelet会通过api service 注册节点自身信息，用于master发现节点,它会定期从etcd获取分配到本机的pod，并根据pod信息启动或停止相应的容器，并定期向master节点汇报节点资源的使用情况,内部集成cadvise来监控容器和节点资源。

2.Kube Proxy:负责为pod提供代理。它会定期从etcd获取所有的service，并根据service信息创建代理。当某个客户pod要访问其他pod时，访问请求会经过本机proxy做转发。底层通过iptables和ipvs来实现

**Etcd？**

Etcd 是用 Go 编程语言编写的一个分布式键值存储，用于协调分布式工作的软件。因此，Etcd 用来存储 Kubernetes 集群的配置数据，这些数据代表在任何给定时间点的集群状态。

### Pod

 Pod是一个逻辑概念，它是Kubernetes资源调度的单元，一般会把一组功能强相关的容器逻辑上称之为一个pod。

Pod 生命周期中的几种不同状态包括pending、running、succeeded、failed、Unknown。

**与API Server交互**

API Server 提供了集群与外部交互的接口，通过 kubectl 命令或者其他 API 客户端提交 pod spec 给 API Server 作为pod创建的起始。Pod 与 API Server 交互的主要流程如下：

1. API Server 在接收到创建pod的请求之后，会根据用户提交的参数值来创建一个运行时的pod对象。
2. 根据 API Server 请求的上下文的元数据来验证两者的 namespace 是否匹配，如果不匹配则创建失败。
3. Namespace 匹配成功之后，会向 pod 对象注入一些系统数据，如果 pod 未提供 pod 的名字，则 API Server 会将 pod 的 uid 作为 pod 的名字。
4. API Server 接下来会检查 pod 对象的必需字段是否为空，如果为空，创建失败。
5. **上述准备工作完成之后会将在 etcd 中持久化这个对象，将异步调用返回结果封装成 restful.response，**完成结果反馈。
6. 至此，API Server 创建过程完成，剩下的由 scheduler 和 kubelet 来完成，此时 pod 处于 pending 状态。

**与scheduler交互**

当提交创建 pod 的请求与 API Server 的交互完成之后，接下来由 scheduler 进行工作，该组件主要是完成 pod 的调度来决定 pod 具体运行在集群的哪个节点上。注意，此处声明一点，**API Server 完成任务之后，将信息写入到 etcd 中，此时 scheduler 通过 watch 机制监听到写入到 etcd 的信息然后再进行工作。**

1. 节点预选：基于一系列预选规则（如 PodFitsResource 和 MatchNode-Selector 等）对每个节点进行检查，将不符合的节点过滤掉从而完成节点预选。
2. 节点优选：对预选出的节点进行优先级排序，以便选出最适合运行 pod 对象的节点。
3. 从优先级结果中挑选出优先级最高的节点来运行 pod 对象，当此类节点多个时则随机选择一个。

注：如果有特殊 pod 资源需要运行在特殊节点上，此时可以通过组合节点标签以及 pod 标签和标签选择器等来实现高级调度，如 MatchInterPodAffinity、MatchNodeSelector 和 PodToleratesNodeTaints 等预选策略，他们为用户提供自定义 Pod 亲和性或反亲和性、节点亲和性以及基于污点及容忍度的调度机制。

**Kubelet组件启动pod**

kubelet 组件的作用不单单是创建 pod，另外还包括节点管理、cAdvisor 资源监控管理、容器健康检查等功能。 

**启动pod流程分析**

kubelet 通过 API Server 监听 etcd 目录，同步 pod 列表。如果发现有新的 pod 绑定到本节点，则按照 pod 清单要求创建 pod，如果是发现 pod 被更新，则做出相应更改。读取到 pod 的信息之后，如果是创建和修改 pod 的任务，则做如下处理：

1. 为该 pod 创建一个数据目录
2. 从 API Server 读取该 pod 清单
3. 为该 pod 挂载外部卷
4. 下载 pod 所需的 Secret
5. 检查已经运行在节点中 pod，如果该 pod 没有容器或者 Pause 容器没有启动，则先停止pod里所有的容器进程。
6. 使用 pause 镜像为每个pod创建一个容器，该容器用于接管 Pod 中所有其他容器的网络。
7. 为 pod 中的每个容器做如下处理：1.为容器计算一个 hash 值，然后用容器的名字去查询对于 docker 容器的 hash 值。若查找到容器，且两者的 hash 值不同，则停止 docker 中容器中进程，并停止与之关联的 pause 容器，若相同，则不做处理。若容器被终止了，且容器没有指定的重启策略，则不做任何处理调用 docker client  下载容器镜像，并启动容器。

### Service

​	在 Kubernetes 中，Pod 是有生命周期的，当 Pod 的生命周期结束之后，Pod会被重新分配 IP。这样就会导致一个问题：在 Kubernetes 集群中，如何让后端业务 Pod(称为 backend)为其他 Pod(称为 frontend)提供服务。

​	为能够使前端发现并连接到后端承载业务的容器组地址，Kubernetes 中提供了 service 服务发现机制，为一组提供相同功能的 Pod 的设定一个相同的标签，service 对象会根据标签为他们提供一个统一的虚拟 IP 作为入口。在 service机制的帮助下，Kubernetes 实现服务发现，负载均衡等机制。service 通过spec.selector 来选取后端服务，一般配合 Replication Controller 或者Deployment 来保证后端容器的正常运行。

​	每个 Pod 在创建时都会拥有一个标签，这个标签由用户自定义。在发布service 对象时，也需要为 service 指定一个标签，service 会寻找所有拥有这个标签的容器组，将这些拥有同样标签的容器组视为提供同样的功能，为它们提供统一的虚拟 service IP 作为对外提供服务的入口。

**Cluster IP** 
	Cluster  IP 主要在每个 node 节点使用 iptables，将发向 Cluster  IP 对应端口的数据，转发到 kube-proxy 中。然后 kube-proxy 通过iptables或ipvs查询到这个 service 下对应 pod 的地址和端口，进而把数据转发给对应的 pod 的地址和端口。 这种类型的 service 会提供一个集群内部的虚拟 IP(与 Pod 不在同一网段)，这个虚拟 IP 仅仅只能供集群内部的 Pod 之间互相通信使用。 
**Node Port** 
	Node Port 是建立在 Cluster IP 的基础之上，在工作节点上打开一个端口，由 kube-proxy 修改 iptables，将访问这个端口的流量转发到对应的后端容器中。即外部流量访问<Node IP>:Node Port 的流量会被转发到 Cluster  IP 然后再转发到 Pod IP。 
**Load Balancer** 
	Load Balancer 建立在 node Port 的基础之上。区别在于 Load Balancer 会把流量根据外部 Load Balancer 定义的规则转发到某个工作节点上的 Node Port中，就是可以调用外部 cloud provider 去创建 LB 来向节点导流。

在 Kubernetes 集群中，每个 Node 运行一个 kube-proxy 进程。kube-proxy负责为 Service 实现一种 VIP(虚拟 IP)的形式。 

## Docker

**1 host模式**

宿主机共用一个Network Namespace。容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口。

**2 container模式**

和已经存在的一个容器共享一个Network Namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的IP，而是和一个指定的容器共享IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过lo网卡设备通信。

**3 none模式**

拥有自己的Network Namespace。但是，并不为Docker容器进行任何网络配置，需要我们自己为Docker容器添加网卡、配置IP等。

**4 bridge模式**

bridge模式是Docker默认的网络设置，此模式会为每一个容器分配Network Namespace、设置IP等，并将一个主机上的Docker容器连接到一个虚拟网桥上。下面着重介绍一下此模式。

# 虚拟化容器融合方案

目前已有方案两种，容器部署在虚拟机中，性能损耗过大；虚拟机和容器物理隔离导致数据中心内出现资源墙
的问题。在大数据与虚拟化超融合技术平台项目中，提出了一种新的虚拟机与容器融合的方案，设计思路如下：  

1. 虚拟机与容器可共存于物理节点，且容器直接运行于物理机之上，虚拟机与容器共享物理机资源，减少性能损耗。  
2. 实现底层资源池统一管理，由云计算的Iaa S层平台对虚拟机与容器资源进行统一分配，实现异构资源的联合调度，打破数据中心资源墙。基于上述方案架构，IaaS层采用Open Stack为底层资源管理工具，PaaS层采用Kubernetes为容器编排工具。

这个容器虚拟机的融合平台中，虚拟机和容器的虚拟网络均由 Neutron 实现，体现了网络资源融合的特点。

为了充分发挥 Kubernetes 提供对容器编排的负载均衡的功能，有必要在openstack中提供类似service的服务 。load balancer和service在功能和结构上有一定相似之处，可以选择用 load balancer 实现 service 。两者的对应关系如下 

| Kubernetes  | Open Stack           | 描述               |
| ----------- | -------------------- | ------------------ |
| Pod         | 虚拟机实例，容器实例 | 后端服务           |
| Pod network | Subnet               | 三层网络           |
| Service     | Load balancer        | 流量转发和负载均衡 |
| Service IP  | Virtual IP           | 对外提供访问入口   |
| endpoint    | port                 | 用于网络连接       |

## kuryr-kubernetes

用kuryr-kubernetes组件可以实现从service到load balancer的映射

Kuryr kubernetes 包括3个独立可执行程序，分别为Kuryr Controller，Kuryr CNI，Kuryr CNI daemon。
本项目只用了Kuryr-controller。Kuryr Controller 独立运行，启动时会起3个线程来watch pod,service,endpoint这3个k8s API 这三种资源，监听到变化会交给相应的handler处理（LoadBalancerHandler和LBaaSSpecHandler），在对应子网中创建与service名称相同的Load Balancer实例，获取endpoint列表（endpoint存着ENDPOINTS就是service关联的pod的ip地址和端口），将容器添加到LB实例的成员池中。

```python
ef _watch(self, path):
        attempts = 0
        deadline = 0
        while self._running and path in self._resources:
            try:
                retry = False
                if attempts == 1:
                    deadline = time.time() + self._timeout

                if (attempts > 0 and
                   utils.exponential_sleep(deadline, attempts) == 0):
                    LOG.error("Failed watching '%s': deadline exceeded", path)
                    self._alive = False
                    return

                LOG.info("Started watching '%s'", path)
                for event in self._client.watch(path):
                    # NOTE(esevan): Watcher retries watching for
                    # `self._timeout` duration with exponential backoff
                    # algorithm to tolerate against temporal exception such as
                    # temporal disconnection to the k8s api server.
                    attempts = 0
                    self._idle[path] = False
                    self._handler(event)
                    self._idle[path] = True
                    if not (self._running and path in self._resources):
                        return
            except Exception:
                LOG.exception("Caught exception while watching.")
                LOG.warning("Restarting(%s) watching '%s'.",
                            attempts, path)
                attempts += 1
                retry = True
                self._idle[path] = True
            finally:
                if not retry:
                    self._graceful_watch_exit(path)
```

用了一个virtual-kubelet，如果创建pod调度到vk上，那么会调用Provider下的createPod接口（增删改查接口）

将pod信息转化为openstack中对应信息，然后调用openstack goforcloud创建容器接口

