# **相关面试题**

1. 语句执行
    1. MySQL一条语句的执行过程，解析过程？查询缓存？怎么判断是否命中？ 
    2. MySQL的执行计划 
2. 锁
    1. 如何锁一行？如何面对高并发访问的情况
    2. select * from user where id = 1 for update;直到锁定的行事务提交
3. 项目和场景题
    1. 问了一个订单数据表（uid，orderId，time，moblie）每天2000w条，要存1年。应该怎么存？
    2. https://www.jianshu.com/p/84da619ce203



# SQL

连表查询时

inner join（内连接），只保留两张表中完全匹配的结果集。

left join,会返回左表所有的行，右表中没有匹配的记录就设为null。

right join,会返回右表所有的行，左表中没有匹配的记录就设为null。

full join,返回左表和右表中所有没有匹配的行。

https://www.cnblogs.com/lijingran/p/9001302.html

```mysql
//插入  如果是自增主键id 或者有默认值可以不出现在字段
INSERT INTO <表名> (字段1, 字段2, ...) VALUES (值1, 值2, ...);
//改
UPDATE <表名> SET 字段1=值1, 字段2=值2, ... WHERE ...;
//删除
DELETE FROM <表名> WHERE ...;
```

**三范式**

1NF:字段不可分;

2NF:只有一个主键，非主键字段依赖主键;

3NF:非主键字段不能相互依赖;

解释:

1NF:原子性 字段不可再分,否则就不是关系数据库;

2NF:唯一性 一个表只说明一个事物;

3NF:每列都与主键有直接关系，不存在传递依赖;

## explain参数



参数

**id** 

​		表示select子语句执行顺序

​		id相同，执行顺序由上到下；

​		子查询id会递增，id大的先执行（括号里的）

**select_type**

​		SIMPLE 简单select，不包含子查询和union

​		PRIMARY 查询中包含子查询，最外层为PRIMARY

​		SUBQUERY 子查询

​		DERIVED 临时表

​		UNION 若第二个SELECT出现在UNION后，则会标记为UNION

​		UNION RESULT  从UNION表获取结果的SELECT

**table** 哪张表的

**type** 访问类型

​		 从好到坏依次如下：system>const>eq_ref>ref>range>index>ALL

​		一般来说，要优化到range，ref

​		system: 表只有一行记录，const类型特例

​		const：通过索引1次找到，用于比较主键或者unique索引，只匹配一行数据。如将主键置于where列表中，MySQL就能将查询转换为一个常量

​		eq_ref:唯一性索引扫描，对于每个索引键，表中只有一条记录与之匹配，常见于主键或者唯一索引扫描；

​		ref:非唯一性索引扫描，返回匹配某个单独值的所有行。

​			有可能会找到多个符合条件的行，所以属于查找和扫描的混合体

​		range：只检索给定范围的行，使用一个索引来选择行

​					例子：where id > 5;where id in (1, 3, 5);where id between 3 and 5;

​		index:全索引扫描

​				select id from A；

​		ALL : 全表扫描

​				select * from A；

**possible_keys :**

​			 显示可能应用在表中的索引。涉及字段上若存在所以索引，		则该索引将被列出，不一定时机使用

**key：**

​			实际使用的索引。NULL：没建或者失效

​					覆盖索引：查询列被所建的索引覆盖

**key_len：**

​			索引中使用的字节数的最大可能长度，并非实际使用长度。根据		表定义计算而得，不是通过表内检索出的

**ref ：**

​			 显示索引的那一列被使用了

**row：**

​			估算找到所需记录读取的行数

**extra：**

​			using filesort 文件内排序。mysql会对数据使用一个外部的索引排序

​			using temporary 使用临时表保存中间结果，常见于order by/groupby

​			using Index 响应select操作使用了覆盖索引，避免了访问表的数据行。如果同时出现using where，表明索引用来执行索引键值的查找，反之表明索引用来读取数据而非执行查找动作



## SQL执行流程

![](https://mmbiz.qpic.cn/mmbiz_jpg/iaIdQfEric9TzWuuhjqx58LnibzsWR0Pf8x9nVefLe59Q8SBNcZGIGn1VGNFfNUVQyOwQksDoyvIOUJicgzU6ICVLg/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1)

- **连接器：** 身份认证和权限相关(登录 MySQL 的时候)。
- **分析器:**  接下来，会经过分析器，分析SQL 语句，检查语法。
- **优化器：** 按照 MySQL 认为最优的方案去执行。
- **执行器:** 执行语句，然后从存储引擎返回数据。

简单来说 MySQL 主要分为 Server 层和存储引擎层：

- **Server 层**：主要包括连接器、分析器、优化器、执行器等，所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图，函数等，还有一个通用的日志模块 binglog 日志模块。
- **存储引擎**： 主要负责数据的存储和读取，采用可以替换的插件式架构，支持 InnoDB、MyISAM、Memory 等多个存储引擎，其中 InnoDB 引擎有自有的日志模块 redolog 模块。**现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始就被当做默认存储引擎了。**

### 1. 查询语句

```sql
select * from tb_student  A where A.age='18' and A.name=' 张三 ';
```

结合上面的说明，我们分析下这个语句的执行流程：

- **连接器**：先检查该语句是否有权限。
- **分析器**：然后通过分析器进行词法分析，提取 sql 语句的关键元素，提取需要查询的表，需要查询所有的列，查询条件是这个表的 id='1'。然后判断语法错误。
- **执行器**：接下来就是优化器进行确定执行方案，那么优化器根据自己的优化算法进行选择执行效率最好的一个方案（优化器认为，有时候不一定最好）。
    - 上面的 sql 语句，可以有两种执行方案：
        - 先查询学生表中姓名为“张三”的学生，然后判断是否年龄是 18
        - 先找出学生中年龄 18 岁的学生，然后再查询姓名为“张三”的学生。
- 进行权限校验，如果没有权限就会返回错误信息，如果有权限就会调用数据库引擎接口，返回引擎的执行结果。

### 2.更新语句

```sql
update tb_student A set A.age='19' where A.name=' 张三 ';
```

更新时需要写binlog和redo log。流程如下：

- 先查询到张三这一条数据。

- 然后拿到查询的语句，把 age 改为 19，然后调用引擎 API 接口，写入这一行数据，InnoDB 引擎把数据保存在内存中，同时记录 redo log，此时 redo log 进入 prepare 状态，然后告诉执行器，执行完成了，随时可以提交。

- 执行器收到通知后记录 binlog，然后调用引擎接口，提交 redo log 为提交状态。

- 更新完成。

### 3.总结

MySQL 主要分为 Server 层和引擎层：

- Server 层主要包括连接器、查询缓存、分析器、优化器、执行器，同时还有一个日志模块（binlog），这个日志模块所有执行引擎都可以共用，redolog 只有 InnoDB 有。
- 引擎层是插件式的，目前主要包括，MyISAM,InnoDB,Memory 等。
- SQL 等执行过程分为两类，
    - 对于查询等过程：权限校验--->查询缓存--->分析器--->优化器--->权限校验--->执行器--->引擎
    - 对于更新等流程：分析器---->权限校验---->执行器--->引擎--->redo log prepare--->binlog--->redo log commit



---

# 索引（重中之重）

## 索引优点：

* 目的：每次查找数据时把磁盘IO次数控制在一个很小的数量级
    * 每次查询时，利用IO能读取到的最大数据量，划分为一页
* 做法：将随机IO变为顺序IO（因为叶子节点是通过链表连在一起的）

---

## B+树索引

1. B+树是适用于文件系统
2. B+树中的**所有数据均保存在叶子结点**，且**根结点和内部结点均只是充当控制查找记录的媒介，并不代表数据本身**
3. B+ 树每个叶子节点都有一个指针，指向下一个数据，形成一个有序链表。

![image-20210312154955536](http://haoimg.hifool.cn/img/image-20210312154955536.png)

---

**查找**

https://zhuanlan.zhihu.com/p/149287061

1. 首先在根节点进行二分查找，找到一个 key 所在的指针
2. 然后递归查找找到叶子节点
3. 在叶子节点上进行二分查找，找出 key 所对应的记录。

---

**插入**

是否满是通过二叉树的阶数判断的（二叉树的高度M），即一个节点最多存M个子节点

若插入后，字节点的数量大于M，则说明要发生旋转

![image-20210312154913246](http://haoimg.hifool.cn/img/image-20210312154913246.png)



![img](http://haoimg.hifool.cn/img/v2-467b2c27f41bad29b01be13e1e5cd1bb_b.webp)

---

**旋转**

旋转发生在Leaf Page已经满，但是其左右兄弟节点没有满得情况下。这时，B+树不会急于去做拆分页得操作，而是将记录移到所在页得兄弟节点上。

如下图所示，插入键值70，此时B+树不会急于去做拆分页的操作，而是将记录移到所在页的兄弟节点上。旋转操作使B+树减少了一次页的拆分操作，同时树的高度保持不变。通常情况下，**左兄弟**会被首先检查用来做旋转操作。

![image-20210312155233367](http://haoimg.hifool.cn/img/image-20210312155233367.png)

---

**删除**

B+树使用填充因子（fill factor)来控制树的删除变化，50%是填充因子可设的最小值。

* 填充因子设置数据占用page空间的比例。比如设置70%，即数据至少要填充page空间的70%，数据页将在70%填满的时候就会分页。
* 设置填充因子的主要作用是为新数据预留一定的空间，当有新数据时，可以插入到预留的空间里，从而避免分页的发生。

填充因子只在新建索引及page分裂时，预留空间，保持设置的比例。在之后的插入，修改，更新操作时，不再维持这个比例。因为如果维持这个比例，就会造成频繁的分页操作，这就违背了设置填充因子的初衷—-为以后新插入的数据预留空间，降低page分裂的操作。

mysql会在page分裂后，判断page中的数据，如果记录占用的比例小于设置的值（如70%），则会把这个page中的数据移动到其它的page

![image-20210312155425136](http://haoimg.hifool.cn/img/image-20210312155425136.png)



---

## **B+树和B树的对比**

**1. B+** **树的磁盘** **IO** **更低**

B+ 树的内部节点并不存储数据，因此B+数内部节点相对 B 树更小。如果把所有同一内部结点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多。一次性读入内存中的关键字也就越多，相对来说IO读写次数也就降低了。

**2. B+** **树的查询效率更加稳定**

所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。

**3. B+树元素遍历效率高**

**局部最优性：

* B 树在提高了磁盘IO性能的同时并没有解决元素遍历的效率低下的问题
* B+树只要遍历叶子节点就可以实现整棵树的遍历。
* 而且在**数据库中基于范围的查询是非常频繁的**，而 B 树不支持这样的操作（或者说效率太低）。

---

## B+树和红黑树的对比

**1.** **磁盘I/O次数**

* 红黑树的出度为 2，树高较高，查询次数较多
* B+树出度一般都大，层数为1-3，所以查找次数少

**2.** **利用磁盘预读特性和局部性原理**

为了减少磁盘 I/O 操作，磁盘往往不是严格按需读取，而是每次都会预读。预读过程中，磁盘进行顺序读取，顺序读取不需要进行磁盘寻道。每次会读取页的整数倍。

* 预读： 磁盘会找到数据的起始位置，并向后读取一页或几页载入内存中 （找到数据需要寻道时间和旋转时间，非常耗时）
* 局部性原理：当一个数据被用到时，其附近的数据也通常会马上被使用

操作系统一般将内存和磁盘分割成固定大小的块，每一块称为一页，内存与磁盘以页为单位交换数据。**数据库系统将索引的一个节点的大小设置为页的大小，使得一次 I/O 就能读一页，从而完全载入一个节点**。



红黑树是一棵自平衡树，特性是：

- 根节点和Null节点是黑色
- 不存在相邻的红节点
- 根节点到Null节点的路径中黑色结点数相同。

**插入**

1. 由于root到Nil节点的路径中黑色结点数相同。插入结点必为红色。
2. 父黑：不调整
3. 父红叔红：父叔染黑爷染红，并对爷节点继续调正
4. 父红叔黑：如果是LL或者RR，就执行一次LL或RR旋转，调整到LR或者RL之后再执行LR,RL旋转，最后插入节点和爷节点交换颜色。

---

## 各种索引

### 唯一索引

数据列只包含彼此各不相同的值，字段值必须唯一，但是可以为空。唯一索引可以保证数据记录的唯一性

---

### 全文索引

只能在文本类型CHAR,VARCHAR,TEXT类型字段上创建全文索引。

字段长度比较大时，如果创建普通索引，在进行like模糊查询时效率比较低，这时可以创建全文索引。MyISAM和InnoDB中都可以使用全文索引。

---

### 哈希索引

**哈希索引能以 O(1) 时间进行查找，但不支持范围查找，失去了有序性**。

* **无法用于排序与分组**
* **只支持对单条记录的精确查找**，无法用于部分查找和范围查找。

InnoDB 存储引擎有一个特殊的功能叫“自适应哈希索引”，当某个索引值被使用的非常频繁时，会在 B+Tree 索引之上再创建一个哈希索引，这样就让 B+Tree 索引具有哈希索引的一些优点，比如快速的哈希查找。

---

### 聚簇索引

表数据文件本身个B+树索引文件，将数据存储与索引放到了一起，**叶子节点存储完整数据**

* 找到了索引也就找到了数据，数据的物理存放顺序与索引顺序是一致的。
* 只要索引是相邻的，对应的数据一定也是相邻的存放在磁盘上的。

**聚簇索引并不是一种单独的索引类型，而是一种数据的存储方式。**

* 具体细节依赖于其实现方式：Innodb中，索引和数据存放在同一个idb文件。InnoDB通过主键聚集数据。

**因为无法把数据同时存放在两个地方，所以一个表只能有一个聚簇索引。**

* 如果没有定义主键，InnoDB会选择一个**唯一**的非空索引代替。
*  如果没有这样的索引， lnnoDB会隐式定义一个主键来作为聚簇索引。

**聚簇索引的优势：**

* 查询效率高：
    * 一次查询：通过聚簇索引可以直接点查找到数据，相比非聚簇索引需要第二次查询（非覆盖索引的情况下）的效率要高
    * 范围查询友好：聚簇索引的数据是按照大小进行排列的，聚簇索引适合在排序的场合进行使用，非聚簇索引不适合

聚簇索引的缺点；

* 维护成本高：在插入新行或者主键被更新等情况下，导致要进行分页时，维护的成本很高。
    * 在大量插入新行后，选择在负载较低的时间段内，对表的结构进行优化，因为必须被移动的行数据可能会造成碎片。
    * 使用独享表空间可以弱化碎片。
* 不合理的使用导致查询效率慢：
    * 使用UUID作为主键，使数据存储稀疏，在查询时数据散落在磁盘中各个页，需要频繁的去磁盘中查询每个页，会导致出现聚簇索引有可能比全表扫描更慢。假如合理的使用索引，可以使查询尽可能的散落到一个页中，这样的查询效率就会变快了。
        * UUID是可以生成时间、空间上都独一无二的值；
    * 主键较大时，辅助索引将会变大，因为辅助索引的叶子存储的是主键值，过长的主键值，会导致非叶子结点占用更多的物理空间。

---

### 非聚簇索引

MyISAM索引文件与数据文件是分离的，**叶子节点不包含行记录的全部数据**，叶子结点存放的是数据行地址。

* 也就是说先在索引中查询到数据行在磁盘中的地址，再去磁盘中查找数据。

**对于非聚簇索引的查询需要回到聚集索引得到整行数据**，这个过程称之为回表，

---

### 联合索引

**对多个字段同时建立的索引**

![clipboard.png](http://haoimg.hifool.cn/img/bVbmBDQ.png)

组合索引的使用，需要遵循**最左前缀匹配原则（最左匹配原则）**。

* 一般情况下在条件允许的情况下使用组合索引替代多个单列索引使用。



---

### 覆盖索引

**需要查询的值已经在索引树上，可以直接提供查询结果，不需要回表，称为覆盖索引**。

使用覆盖索引的优点：

* **覆盖索引可以减少树的搜索次数**，显著提升查询功能，所以使用覆盖索引是一个常用性能优化手段。
* **辅助索引不包含整行记录的所有信息**，所以其**大小远小于聚集索引**，因此可以**减少大量的IO操作**。

**实现方法：将被查询字段建立到联合索引去。**

---

### **前缀索引**

对于 BLOB、TEXT 和 VARCHAR 类型的列，需要进行全字段匹配或者前匹配（‘xxx’ 或者 like ‘xxx%'），必须使用前缀索引，对文本的前几个字符（具体是几个字符在建立索引时指定）建立索引，这样建立起来的索引更小，所以查询更快。

* 例子：很多行数据的area字段都是以China开头，那么如果以前1-5位字符做前缀索引，就会出现大量索引值重复的情况，
    * 建立合理的前缀索引，索引值重复性越低，查询效率也就越高

前缀长度的选取需要根据索引选择性来确定。

**使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。**

**前缀索引缺点：**

* 可能会增加扫描行数，影响性能
* 使用前缀索引用不上覆盖索引对查询的优化，因为一定会回表确认完整信息。
* MySQL 无法使用前缀索引做 ORDER BY 和 GROUP BY , 也无法使用前缀索引做覆盖扫描。



---

## **适合索引的场景**

查询的时间更快；占用的空间更小

1. 主键
    * 主键一般为id等具有唯一性标识的字段，需要频繁查找、连接。
    * InnoDB中会自动为主键建立聚集索引，即使没有定义主键，也会自动生成一个隐藏主键建立索引；
    * MyISAM中不会自动生成主键。
    * 建议给每张表指定主键。

2. 频繁作为查询条件的字段
    * 数据量很大，频繁进行查询时，建立索引，使用空间换时间
    * 索引是以空间换时间的，某字段如果频繁作为查询条件，建议建立索引使用前缀索引后，可能会导致查询语句读数据的次数变多。

---

## **不适合索引的场景**

1. **唯一性差**：数据重复且分布平均的字段（比如性别、年龄），查出来的数据区分度很低，还需要进行查询，查询效率低
2. **很少作为查询条件**的字段：where中不用的字段;
3. **频繁更新**的字段、表（更新索引消耗）
  * 如果字段添加了索引，在更新时不仅要更新数据本身，还要**维护其索引，如果频繁更新会带来很多额外开销**。
  * 再者，如果一个表频繁进行增删改操作，也不适合索引
4. 尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a, b)的索引，只需要要修改原来的索引为联合索引就ok了
5. 数据量大的字段（比如长字符串、image）：维护索引的开销很大
    1. 索引需要占用物理空间，如果需要简历聚簇索引，那么需要的空间更大，
    2. 如果非聚簇索引很多，一旦聚簇索引改变，那么所有的非聚簇索引都会跟着改变。
6. 表的记录不多
  * 一般数据量达到300万-500万时考虑建立索引。

---

## 索引失效

https://juejin.cn/post/6960927821241778212

**符合最左前缀原则**。联合索引a,b,c，b+ 树是按照从左到右的顺序比较的，a，b，c

1. where条件有**or**
    1. or 可能会导致索引失效，并非一定，这里涉及到MySQL **index merge** 技术。
2. **联合索引中，where中索引列违背最左匹配原则;**
3. 1. 如果是组合索引的话，如果不按照索引的顺序进行查找，直接使用第三个位置上的索引，而忽略第一二个位置上的索引时，则会进行全表查询
4. **前导模糊查询**不能利用索引(where XX like '%XX' 或者 like '%XX%')
    1. 如果是这样的条件where code like 'A % '，就可以查找CODE中A开头的CODE的位置。当碰到B开头的数据时，就可以停止查找了，因为后面的数据一定不满足要求。这样就可以利用索引了。
5. **索引无法存储null值**，所以where的判断条件如果对字段进行了null值判断，将导致数据库放弃索引而进行全表查询
    1. select id from t where num is null，
    2. 单列索引无法储null值，复合索引无法储全为null的值。
    3. 查询时，采用is null条件时，不能利用到索引，只能全表扫描。
6. **避免在 where 子句中使用!=或< >操作符**，否则将引擎放弃使用索引而进行全表扫描
7. **in 和 not in 也要慎用**，否则会导致全表扫描
    1. select id from t where num in(1,2,3)
8. 需要**类型转换**; （varchar类型传了数字）
    1. select * from t_user where user_name = 100;
    2. 因为user_name 字段定义的是varchar，索引在where进行匹配时会先隐式调用 case() 函数进行类型转换 将匹配条件变成，**user_name = '100'**
9. where里索引列**使用了函数,表达式;**
    1. select id from t where substring(name,1,3)='abc'
    2. select id from t where num/2=100 
    3. 应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。

---

## 怎么建立索引

1. 最左前缀匹配原则
    1. mysql会一直向右匹配，直到遇到范围查询(>、<、between、like)就停止匹配，
    2. 比如，where a = 1 and b = 2 and c > 3 and d = 4 
        1. 如果建立(a,b,c,d)顺序的索引，d是用不到索引的
        2. 如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。
2. .=和in可以乱序，
    1. 比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式。
3. 尽量选择区分度高的列作为索引，
    1. 区分度的公式是count(distinct col)/count(*)，表示字段不重复的比例，比例越大我们扫描的记录数越少，唯一键的区分度是1，而一些状态、性别字段可能在大数据面前区分度就是0，
    2. 这个比例有什么经验值吗？使用场景不同，这个值也很难确定，一般需要join的字段我们都要求是0.1以上，即平均1条扫描10条记录。
4. 索引列不能参与计算，保持列“干净”，
    1. 比如from_unixtime(create_time) = ’2014-05-29’就不能使用到索引，
    2. 原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。
    3. 所以语句应该写成create_time = unix_timestamp(’2014-05-29’)。
5. 尽量的扩展索引，不要新建索引。
    1. 比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。

---

# 日志

MySQL数据库可以实现主从复制，实现持久化，实现回滚，其实关键在于MySQL里的三种`log`，分别是：

- binlog：主要用于复制和数据恢复。
- redo log：用于恢复在内存更新后，还没来得及刷到磁盘的数据。
- undo log：用于实现回滚和多版本控制。

## **binlog**

* 事务提交时，用于**记录数据库表结构和表数据变更的二进制日志**，比如insert、update、delete、create、truncate等等操作。
    * 记录下每条变更的sql语句，还有执行开始时间，结束时间，事务id等等信息。
    * 不会记录select、show操作，因为没有对数据本身发生变更。
* 用于主从复制，从库订阅主库上的binlog进行重播，实现主从同步。
* 用于数据恢复，基于时间范围，还原指定区间的数据。



## **redo log**

* mysql修改数据的步骤：
    1. MySQL先把这条记录所在的页加载到内存中，然后对记录进行修改。
    2. MySQL持久化时，需要修改的数据刷到磁盘中，这时数据库宕机了，那么这次修改成功后的数据就丢失了。
* redo log 是物理格式的日志，记录数据库每个页的修改信息。
* 当执行数据变更操作时，redo log：
    1. 首先把数据也加载到内存中
    2. 然后在内存中进行更新
    3. 更新完成后写入到redo log buffer中，然后由redo log buffer在写入到redo log file中。
* redo log file记录着xxx页做了xxx修改，所以即使mysql发生宕机，也可以通过redo log进行数据恢复。
    * 也就是说在内存中更新成功后，即使没有刷新到磁盘中，但也不会因为宕机而导致数据丢失。
* binlog和redo log都可以数据恢复，有什么区别？
    * redo log所记录的待恢复的数据：在内存更新后，还没来得及刷到磁盘的数据。
        * redo log不会存储历史所有的数据的变更
        * 当内存数据刷新到磁盘中，redo log的数据就失效了，内容会被覆盖。
    * binlog是存储所有数据变更的情况，理论上只要记录在binlog上的数据，都可以恢复。

InnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，总共就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示。

![image-20210312165352962](http://haoimg.hifool.cn/img/image-20210312165352962.png)

* write pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。
* checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。
* write pos 和 checkpoint 之间的是“粉板”上还空着的部分，可以用来记录新的操作。如果 write pos 追 上 checkpoint ，表示“粉板”满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把checkpoint 推进一下。



## **undo log**

* **用于回滚**
    * 用undo log的日志就可以恢复到修改前的数据。
    * **事务的原子性是通过undo log实现的。**
    * undo log主要存储的是**数据的逻辑变化日志**，**记录的是数据修改之前的数据**
        * 比如说我们要`insert`一条数据，那么undo log就会生成一条对应的delete日志。
* 另一个作用是**实现多版本控制(MVCC)**
    * undo记录中包含了**记录更改前的镜像，**
    * **如果更改数据的事务未提交**，对于隔离级别大于等于read commit的事务而言，**不应该返回更改后数据，而应该返回老版本的数据**。



---

# 锁机制



基于锁的属性分类：

1. 读锁：共享锁
2. 写锁：排他锁

基于锁的粒度分类：

1. 行级锁
2. 表级锁
3. 页级锁
4. 记录锁
5. 间隙锁
6. 临键锁

基于锁的状态划分：

1. 意向共享锁
2. 意向排他锁

---

## **读/写锁(share / exclusive)**

* 读锁（共享锁Share Locks，记为S锁）：针对同一份数据，多个读操作可以同时进行而不会相互影响  
    * 共享锁之间不互斥，简记为：**读读可以并行**
* 写锁（排他锁eXclusive Locks，记为X锁）：当前写操作没有完成前，会阻断其他写锁和读锁。
    * 排他锁与任何锁互斥，简记为：**写读，写写不可以并行**

**共享/排它锁的潜在问题是，不能充分的并行，解决思路是数据多版本**

---

## 悲观锁与乐观锁

都是为了解决并发写数据库时的一致性问题。本质上是对锁机制的一种看待角度。

乐观锁

- 乐观锁假认为数据不会被其他事务修改，只在update时检测是否冲突。
- 实现：版本控制+行锁。
    - 添加version字段。
    - 更新时版本号+1.
    - 提交版本必须大于记录当前版本才能执行更新



悲观锁

* 认为数据会被别的事务修改，整个数据操作过程都加锁。
* 悲观锁有共享锁和排他锁。
    * 共享锁是select … lock in share mode 
    * 排他锁是select...for update（不加的话就是快照读）

```mysql
//设置数据库不自动提交
set autocommit = 0;
//操作1 执行
select * from test where id = 1 for update; 
//操作2 执行
update test set age = 22 where id = 1;
commit;
```

---

## **行锁和表锁**：

**表锁：**

* 表锁是指上锁之后，锁住的是整个表，下一个事物访问该表时，必须等当前事物释放了锁，才能对表进行访问
* 力度大，发生锁冲突概率高，并发度低。
* MyIsam的读写调度是写优先，大量更新会使查询很难得到锁，从而造成永远阻塞，因此myisam不适合做写为主表的引擎。
* 在不通过索引条件查询的时候，InnoDB使用的是表锁，而不是行锁。 

**行锁：**

* 粒度小，加锁闭表锁麻烦，不容易冲突，相比表锁的并发度高。
* **InnoDB 行锁是通过给索引上的索引项加锁来实现的。**
    * **通过索引条件检索数据，InnoDB 才使用行级锁，否则，InnoDB 将使用表锁！**
    * 在 InnoDB 事务中，**行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放**。
        * 这个就是两阶段锁协议。如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放，这样能够减少锁持有的时间。
* varchar类型没加引号，会导致索引失效，行锁变表锁

---

## **记录锁（Record Lock)**

* 记录锁属于行锁的一种，记录锁锁住的范围只是表中的某一条记录
* 精确条件命中，命中的条件字段时唯一索引
* **避免**在查询的时候的**重复读问题**，避免了在修改的事务未提交前，被其他事务**脏读的问题**

它封锁索引记录，例如：

```sql
select * from t where id=1 for update;
```

它会在id=1的索引记录上加锁，以阻止其他事务插入，更新，删除id=1的这一行。

---

## 页锁

* 页锁时mysql中，锁定粒度介于表级锁以及行级锁之间的锁，
    * 表级锁速度快，但是冲突多
    * 行级锁速度慢，冲突少
* 页锁一次锁定相邻的一组记录。
* 开销介于表锁和记录锁之间
* 可能会出现死锁

---

## **间隙锁（Gap Lock）**

* 属于行锁的一种，间隙锁锁加在一段不存在的空闲区间，可以是两个索引记录之间，也可能是第一个索引记录之前（负无穷，first-key）或最后一个索引之后(last-key，正无穷）的无限空间。
* 遵循左开右闭原则，锁定过程中无法插入范围内任何数据。

```sql
select * from t where id between 8 and 15 **for update**;
```

用范围条件检索数据，对于**键值在条件范围内但不存在的记录**，叫做间隙GAP，InnoDB也会对这个间隙加锁，即间隙锁。

* **目的是为了防止幻读，事务并发时，如果没有间隙锁，会直接在一段区间内插入数据，造成幻读现象。**

---

## **临键锁 (Next-Key Locks)**

* **临键锁**，行锁的一种，是记录锁 record lock 与间隙锁 gap lock 的组合。
* **临键锁会封锁索引记录本身，以及索引记录两边之前的间隙。**
* 在Next-key Lock算法下，InnoDB对于行的查询都是采用这种锁定算法。
* 临键锁**避免了**在范围查询时出现**脏读、重复读、幻读问题**。

---

## 自增锁（Auto-inc Locks）

**自增锁是一种特殊的表级别锁（table-level lock）**，专门针对事务插入 AUTO_INCREMENT 类型的列。

* 最简单的情况，如果一个事务正在往表中插入记录，所有其他事务的插入必须等待，以便第一个事务插入的行，是连续的主键值。

---

## 意向锁（Intention Lock)

**背景：**

1. 事务A对表C加了一把记录锁后，
2. 事务B想对表C的加一把表锁，那么事务B就需要查看该表的索引树中，是否存在别的事务对记录加了记录锁，
3. 这个过程开销非常的大。

**意向锁，是一个表级别的锁**(table-level locking)

1. 当事务A对表的记录加锁成功后，就**设置一种状态通知后面的事务，已经有事务对表的记录加了排他锁了**，其他的事务不可以对整个表加共享锁或者是排他锁了，
2. **后面的事务只需要查看这种状态就知道是否可以对表进行加锁，**
3. **避免了对整个索引树的每个节点进行扫描是否加锁**，这个状态就是意向锁。

InnoDB支持多粒度锁(multiple granularity locking)，它允许行级锁与表级锁共存，实际应用中，InnoDB使用的是意向锁。

**意向锁分为**：

1. **意向共享锁(IS Lock)**，事务想要获得一张表中某几行的共享锁S，**首先需要获得表的意向共享锁IS**
2. **意向排他锁(IX Lock)**，事务想要获得一张表中某几行的排他锁X，**首先需要获得表的意向排他锁IX**



1. 由于意向锁仅仅表明意向，它其实是比较弱的锁，**意向锁之间并不相互互斥**，而是可以并行

    ![image-20210312220728839](http://haoimg.hifool.cn/img/image-20210312220728839.png)

2. 它会与共享锁/排它锁互斥，其**兼容互斥表**如下：

    ![image-20210312220743063](http://haoimg.hifool.cn/img/image-20210312220743063.png)



---

## **破坏死锁**

什么是死锁？

* 死锁：多个线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放，最终导致线程无限期被阻塞。

![线程死锁示意图 ](http://haoimg.hifool.cn/img/68747470733a2f2f6d792d626c6f672d746f2d7573652e6f73732d636e2d6265696a696e672e616c6979756e63732e636f6d2f323031392d342f323031392d34254536254144254242254539253934253831312e706e67.png)



### 1 产生死锁的条件

同时满足以下四种条件则会产生死锁：

1. **互斥条件**：该资源任意一个时刻只由一个线程占用。
2. **请求与保持条件**：一个进程因请求资源而阻塞时，对已获得的资源保持不放。
3. **不剥夺条件**：线程已获得的资源在未使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。
4. **循环等待条件**：若干进程之间形成一种头尾相接的循环等待资源关系。

### 2 消除死锁

为了避免死锁，只要破坏产生死锁的四个条件中的其中一个就可以了：

1. **破坏互斥条件** ：
    1. 这个条件不能被破坏，因为锁是使临界资源互斥访问的。
2. **破坏请求与保持条件** ：
    1. 要一次性申请所有的资源。
3. **破坏不剥夺条件** ：
    1. 占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。
4. **破坏循环等待条件** ：
    1. 靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。



---

**1.** **进入等待，超时释放资源**

这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。

* **不能太大**。在 InnoDB 中， innodb_lock_wait_timeout 的默认值是 50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过 50s 才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。
* **不能太小**。比如 1s。这样当出现死锁的时候，确实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会出现很多误伤。

正常情况下我们还是要采用第二种策略，死锁检测。

**2.** **死锁检测**

InnoDB采用wait-for graph（等待图）方式来进行死锁检测。

wait-for graph要求数据库保存以下两种信息：

* **锁的信息链表**
* **事务等待链表**

通过上述链表可以构造出一张图，如果这个图中存在回路，就代表存在死锁，资源间相互发生等待。

wait-for graph中，图中的节点是事务，对节点之间边的定义如下，

* 事务T1等待事务T2释放锁
* 事务T1发生在事务T2之后

以下图为例:

![image-20210312225020281](http://haoimg.hifool.cn/img/image-20210312225020281.png)

上图包含的信息如下，

transaction wait lists中存在4个事务，所以在wait-for graph中也会有四个节点。

* 事务t2对记录row1使用了X lock锁定该行数据。
* 事务t1对记录row2使用了S Lock锁定该行数据。
* 事务t1和t2的锁不兼容，所以t1在等待事务t2释放row1上的锁，故存在一条从t1指向t2的边
* 事务t2的X lock与任何锁都不兼容，所以t2在等待事务t1和t4释放记录row2上的锁，故存在从t2指向t1和t4的边

由此得到如下wait-for graph，

![image-20210312225128183](http://haoimg.hifool.cn/img/image-20210312225128183.png)

图中可以发现节点t1和t2直接存在回路，故这两个节点对应的事务存在死锁。

wait-for graph是主动监测死锁的机制，通常采用深度优先算法实现。一旦发现存在死锁，在两个事务中**选择undo成本低的事务进行回滚**。



死锁检测的缺点:

* 想象有 1000 个并发线程要同时更新同一行，那么死锁检测操作就是 100 万这个量级的。**最终检测的结果是没有死锁，却耗费了大量的CPU资源。**
* **解决方法**：
    * 如果能确保业务一定不会死锁，**临时关闭死锁检测**。**关闭死锁有风险！ 可能会大量超时！**
    * **控制并发度**。在服务端控制并发度。在客户端控制并发度没啥用，比如600个客户端每个客户端有2个请求，到达服务端后就有1200个请求，还是大。可以通过中间件实现，也可以有高手能够修改MySQL源码。
    * **拆分记录**。将一行改成逻辑上的多行来减少锁冲突。比如一个电影账户，拆成10个记录的值的和，每次选择随机一条记录来加。这样冲突概率变成原来的1/10

---

# MVCC - 多版本并发控制

https://zhuanlan.zhihu.com/p/265280977

- 为什么需要MVCC？
    * 不通过锁，提高读写时，多线程之间不冲突的并发度。
- InnoDB的MVCC是通过在每行记录后面保存两个隐藏的列来实现的。**一个保存了行的事务ID（DB_TRX_ID），一个保存了行的回滚指针（DB_ROLL_PT）**。
- 读取数据时通过版本号将数据保存下来，不同的事务session会看到自己特定版本的数据。
    - InnoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id 。
    - transaction id是在事务开始的时候向InnoDB 的事务系统申请的，是按申请顺序严格递增的。
- MVCC只在 READ COMMITTED 和 REPEATABLE READ 两个隔离级别下工作。
    - 其他两个隔离级别够和MVCC不兼容
    - READ UNCOMMITTED 总是读取最新的数据行, 而不是符合当前事务版本的数据行。
    - SERIALIZABLE 则会对所有读取的行都加锁。





## **MVCC快照读**

如果数据版本是启动之前生成的，就认；如果是启动之后生成的就不认。

**在实现上:**

- InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。

    - “活跃”指的就是，当前事物已经启动了，但还没提交。

- 数组里面**事务 ID 的最小值记为低水位**，当前系统里面已经创建过的**事务 ID 的最大值加 1 记为高水位**。

    - **对于每个事务来说，版本号凡是在本事物的“活跃事务数组”里的数据版本，当前事务不认。**

- 这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。

    - 而数据版本的可见性规则，就是基于数据的 row trx_id 和这个一致性视图的对比结果得到的。

- 这个视图数组把所有的 row trx_id 分成了几种不同的情况。

    ![image-20210312221728071](http://haoimg.hifool.cn/img/image-20210312221728071.png)



对于当前事务的启动瞬间来说，一个数据版本的 row trx_id，有以下几种可能：
1. 如果落在**绿色**部分，表示这个版本是**已提交的事务或者是当前事务自己生成**的，这个**数据是可见**的；
2. 如果落在**红色**部分，表示这个版本是由**将来启动的事务生成**的，是肯定不可见的；
3. 如果落在**黄色**部分，那就包括两种情况
1. 若 row trx_id **在数组中**，表示这个版本是由**还没提交的事务生成**的，**不可见**；
2. 若 row trx_id **不在数组**中，表示这个版本是**已经提交了的事务生成的**，**可见**。



* Read Commited 下，事务在每次Read操作时，都会建立Read View
* Repeatable Read 下，事务在第一个Read操作时，会建立Read View，之后会复用之前的Read View

---

## **MVCC更新逻辑**

更新数据都是先读后写的，而这个读，只能读当前的值，称为当前读（current read）

* 当前读能够看到当前事务之后提交事务修改的数据。
* 如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。

---

# 事务

## **ACID 属性**  

1. **原子性（Atomicity）**
    * 事务的所有操作要么全部提交成功，要么全部失败回滚。
    * **回滚可以用回滚日志（Undo Log）来实现**，回滚日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。

2. **一致性（Consistency）**
    * 数据库在事务执行前后都保持一致性状态。  
    * 例如，在一个表中有一个字段为姓名，为唯一约束，即在表中姓名不能重复。如果某个事务对姓名字段进行了修改，但是在事务提交或事务操作发生回滚后，表中的姓名变得非唯一了，即数据库从一种状态变为了一种不一致的状态。
    * **利用undo log回滚保证一致性**

3. **隔离性（Isolation）**
    * 一个事务所做的修改在最终提交以前，对其它事务不可见。 
    * **Innodb利用MVCC保证可重复读**

4. **持久性（Durability）**
    * 一旦事务提交，所做的修改将会永远保存到数据库中。 
    * **利用redo log保证持久性**

---

## **事务并发可能出现的情况**

**产生并发不一致性问题的主要原因是破坏了事务的隔离性，解决方法是通过并发控制来保证隔离性。**

* 并发控制可以通过封锁来实现，但是封锁操作需要用户自己控制，相当复杂。
* 数据库管理系统提供了事务的隔离级别，让用户以一种更轻松的方式处理并发一致性问题。

### **脏读**

![image-20210312213721882](http://haoimg.hifool.cn/img/image-20210312213721882.png)

* **A事务读取B事务尚未提交的数据**，
* 此时如果B事务发生错误并执行回滚操作，
* 那么A事务读取到的数据就是脏数据。

### **不可重复读**

![image-20210312213740248](http://haoimg.hifool.cn/img/image-20210312213740248.png)

* **原来读到了，但是再次读的时候，已有的数发生了变化**
* 一个事务只能读到另一个已提交事务修改过的数据，并且其他事务每对该数据进行一次修改并提交后，该事务都能查询得到最新值。（不可重复读在读未提交和读已提交隔离级别都可能会出现）
* 一个事务读取同一条记录2次，得到的结果不一致：

### **幻读**

![image-20210312213811316](http://haoimg.hifool.cn/img/image-20210312213811316.png)

* 一个事务按同一条件读取2次，得到的记录条数不一致，**读到了没有的数据**
* 一个事务先根据某些条件查询出一些记录，之后另一个事务又向表中插入了符合这些条件的记录，原先的事务再次按照该条件查询时，能把另一个事务插入的记录也读出来。
* 幻读和不可重复读有些类似，但是**幻读强调的是集合的增减，而不是单条数据的更新。**
* **MySQL使用行锁和间隙锁配合防止幻读现象。**



### 第一类更新丢失

* 事务A和事务B都对数据进行更新，但是事务A由于某种原因事务回滚了，把已经提交的事务B的更新数据给覆盖了。

![img](https://pic1.zhimg.com/50/v2-df373501c48e4bba633c859944394e53_720w.jpg?source=1940ef5c)



### 第二类更新丢失

* 两个事务同时对数据进行更新，但是事务A的更新把已提交的事务B的更新数据给覆盖了。

![img](https://pic2.zhimg.com/80/v2-b865701afea8e74b2370187c4837da49_1440w.jpg?source=1940ef5c)

---

## 事务的隔离级别

事务隔离级别有四种，未提交读，已提交读，可重复读和序列化。

* **MySQL 默认是可重复读。**
* **可重复读**，采用Next-Key算法，能够避免幻读。





### **读未提交**(read uncommited)

* 事务A可以读到事务B已修改未提交的数据，并在此基础上做了操作。如果事务B回滚，则A的数据无效。
* **只能防止第一类更新丢失，不能解决脏读，可重复读，幻读**，所以很少应用于实际项目。

### **读已提交(read committed)**

* 一个事务的更新操作结果只有在**该事务提交之后，另一个事务才可能读取到同一笔数据更新后的结果**。
* **Oracle 和 SQL Server 的默认隔离级别。**
* 解决了脏读和第一类更新丢失，但会发生不可重复读、幻读。

### **可重复读(repeatable read)**

* 一个事务多次读同一个数据，**在这个事务还没结束时，其他事务不能访问该数据(包括了读写)**，这样就可以在同一个事务内两次读到的数据是一样的。
* 解决了脏读、不可重复读、第一类更新丢失、第二类更新丢失的问题，
    * **还是会出现幻读**
* **MySQL 默认的隔离级别**
* **INNODB的解决方案是MVCC+临键锁（ next-key locking ）。**
    * 间隙锁使得InnoDB不仅仅锁定查询涉及的行，还会对索引中的间隙进行锁定，以防止幻影行的插入

### **串行化(serializable)**

* 对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。
* 事务序列化执行，事务只能一个接着一个地执行，不能并发执行。
* 在这个级别，可能导致大量的超时现象和锁竞争。



![img](http://haoimg.hifool.cn/img/v2-25ed812ff748a38bd3e4127db1ed7a48_1440w.jpg)

---

## 分布式事务

在分布式系统中实现事务，由多个本地事务组合而成。

对于分布式事务而言几乎满足不了 ACID，其实对于单机事务而言大部分情况下也没有满足 ACID，不然怎么会有四种隔离级别呢？

### 2PC

2PC（Two-phase commit protocol）。 **二阶段提交是一种强一致性设计**，2PC 引入一个事务协调者的角色来协调管理各参与者（也可称之为各本地资源）的提交和回滚，二阶段分别指的是准备（投票）和提交两个阶段。

2PC 是一种**尽量保证强一致性**的分布式事务，因此它是**同步阻塞**的，而同步阻塞就导致长久的资源锁定问题，**总体而言效率低**，并且存在**单点故障**问题，在极端条件下存在**数据不一致**的风险。适用于**数据库层面的分布式事务场景**

### **3PC**

相比于 2PC 它在**参与者中也引入了超时机制**，并且**新增了一个阶段**使得参与者可以利用这一个阶段统一各自的状态。

3PC 包含了三个阶段，分别是**准备阶段、预提交阶段和提交阶段**

### **TCC**

**2PC 和 3PC 都是数据库层面的，而 TCC 是业务层面的分布式事务**

TCC 指的是`Try - Confirm - Cancel`。

- Try 指的是预留，即资源的预留和锁定，**注意是预留**。
- Confirm 指的是确认操作，这一步其实就是真正的执行了。
- Cancel 指的是撤销操作，可以理解为把预留阶段的动作撤销了。



### **本地消息表**

本地消息表就是会有一张存放本地消息的表，一般都是放在数据库中，然后在执行业务的时候 **将业务的执行和将消息放入消息表中的操作放在同一个事务中**，这样就能保证消息放入本地表中业务肯定是执行成功的。

然后再去调用下一个操作，如果下一个操作调用成功了好说，消息表的消息状态可以直接改成已成功。

如果调用失败也没事，会有 **后台任务定时去读取本地消息表**，筛选出还未成功的消息再调用对应的服务，服务更新成功了再变更消息的状态。

这时候有可能消息对应的操作不成功，因此也需要重试，重试就得保证对应服务的方法是幂等的，而且一般重试会有最大次数，超过最大次数可以记录下报警让人工处理。

可以看到本地消息表其实实现的是**最终一致性**，容忍了数据暂时不一致的情况。





### **消息事务**

RocketMQ 就很好的支持了消息事务，让我们来看一下如何通过消息实现事务。

第一步先给 Broker 发送事务消息即半消息，**半消息不是说一半消息，而是这个消息对消费者来说不可见**，然后**发送成功后发送方再执行本地事务**。

再根据**本地事务的结果向 Broker 发送 Commit 或者 RollBack 命令**。

并且 RocketMQ 的发送方会提供一个**反查事务状态接口**，如果一段时间内半消息没有收到任何操作请求，那么 Broker 会通过反查接口得知发送方事务是否执行成功，然后执行 Commit 或者 RollBack 命令。

如果是 Commit 那么订阅方就能收到这条消息，然后再做对应的操作，做完了之后再消费这条消息即可。

如果是 RollBack 那么订阅方收不到这条消息，等于事务就没执行过。

可以看到通过 RocketMQ 还是比较容易实现的，RocketMQ 提供了事务消息的功能，我们只需要定义好事务反查接口即可。

![img](http://haoimg.hifool.cn/img/v2-72ba7bed684e855606c44ddda185987d_1440w.jpg)

可以看到消息事务实现的也是最终一致性。

### **最大努力通知**

其实我觉得本地消息表也可以算最大努力，事务消息也可以算最大努力。

就本地消息表来说会有后台任务定时去查看未完成的消息，然后去调用对应的服务，当一个消息多次调用都失败的时候可以记录下然后引入人工，或者直接舍弃。这其实算是最大努力了。

事务消息也是一样，当半消息被commit了之后确实就是普通消息了，如果订阅者一直不消费或者消费不了则会一直重试，到最后进入死信队列。其实这也算最大努力。

所以**最大努力通知其实只是表明了一种柔性事务的思想**：我已经尽力我最大的努力想达成事务的最终一致了。

适用于对时间不敏感的业务，例如短信通知。

### **总结**

可以看出 2PC 和 3PC 是一种强一致性事务，不过还是有数据不一致，阻塞等风险，而且只能用在数据库层面。

而 TCC 是一种补偿性事务思想，适用的范围更广，在业务层面实现，因此对业务的侵入性较大，每一个操作都需要实现对应的三个方法。

本地消息、事务消息和最大努力通知其实都是最终一致性事务，因此适用于一些对时间不敏感的业务。



---

# 主从同步

MySQL主从复制涉及到三个线程：

* 主节点 master
    * log dump thread：
        * binlog：主从复制的基础是主库记录数据库的所有的变更记录到binlog。
            * binlog是数据库服务器启动的那一刻起，保存所有的修改数据库结构或内容的一个文件。
        * Log dump thread：当bin log 有改动时，log dump线程读取其内容，发送给从节点
* 从节点 slave
    * I/O thread：接收binlog的内容，并将其写入到relay log中
    * SQL thread：读取relay log中的内容，对数据更新进行重放，最终保证主从数据库的一致性
* 主从节点使用binlog + position 偏移量来定位主从同步的位置，从节点会保存其已接收到的偏移量，如果从节点发生宕机，则会自动从position的位置开始发起同步

![img](http://haoimg.hifool.cn/img/v2-1b0c3f31bd398c39b9e0930059b0ca24_720w.jpg)



**配置：**

- 主服务器：

  - 开启二进制日志bin log
  - 配置唯一的server-id
  - 获得master二进制日志文件bin log名及位置position
  - 创建一个用于slave和master通信的用户账号

- 从服务器：

  - 配置唯一的server-id
  - 使用master分配的用户账号读取master二进制日志 bin log
- 启用slave服务


---

## 读写分离

主服务器处理写操作以及实时性要求比较高的读操作，而从服务器处理读操作。

读写分离能提高性能的原因在于：

- 主从服务器负责各自的读和写，极大程度缓解了锁的争用；
- 从服务器可以使用 MyISAM，提升查询性能以及节约系统开销；
- 增加冗余，提高可用性。

读写分离常用代理方式来实现，代理服务器接收应用层传来的读写请求，然后决定转发到哪个服务器。

[![img](https://camo.githubusercontent.com/ff6ad220f8fcad4c96fdf5415c2ada0aa94c16f1/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f6d61737465722d736c6176652d70726f78792e706e67)](https://camo.githubusercontent.com/ff6ad220f8fcad4c96fdf5415c2ada0aa94c16f1/68747470733a2f2f63732d6e6f7465732d313235363130393739362e636f732e61702d6775616e677a686f752e6d7971636c6f75642e636f6d2f6d61737465722d736c6176652d70726f78792e706e67)



---

## **主从延迟问题**

刚写入主库的数据读不到怎能么办？

1. 半同步复制：master写入binlog日志之后，就会将强制此时立即将数据同步到slave，slave将日志写入自己本地的relay log之后，接着会返回一个ack给主库，主库接收到至少一个从库的ack之后才会认为写操作完成了。 
2. 并行复制：slave开启多个线程，并行读取relay log中不同库的日志，然后并行重放不同库的日志，这是库级别的并行。 缓解主从延迟
3. redis做缓存？

 https://www.cnblogs.com/windpoplar/p/11978742.html

多个读多个写怎么进行？



---

## 主库宕机

现象：由于mysql 默认的方式是异步的，主库把日志发送给从节点后，主库并不关系从节点是否以及处理这些日志。这时，如果主节点宕机了，从节点处理这些日志失败了，那么这些日志就丢失了。

解决办法：

1. 全同步复制：主库写入bin log后，强制同步日志到从库，所有的从库都执行完成之后，再把结果返回给客户端。但是这样的性能很低
2. 半同步复制：从库写入日志之后，返回ack确认成功信息给主库，主库收到了至少一个ack后，就确认写入成功。
3. **？？？用zookeeper对数据库集群进行管理，主库宕机时选举主库？？？**



---

# 引擎

## 存储结构

**MYSQL基本存储结构是页，数据页组成双链表，页中记录组成单链表。**

* 查询过程是**先遍历页双向链表**，**再遍历记录单链表**

## InnoDB

InnoDb是 MySQL 默认的事务型存储引擎。

* **InnoDb被设计成为大量的短期事务，短期事务大部分情况下是正常提交的，很少被回滚。**
* InnoDB的性能与自动崩溃恢复的特性，使得它在非事务存储需求中也很流行。

**InnoDb索引类型：**

* 聚簇索引，有且仅有一个，主键。
* 普通索引，多个，叶子节点存储主键值。
    * 普通索引查询过程(回表查询，先定位主键，再定位行记录）：
        * 先遍历普通索引B+树获得主键值；
        * 然后遍历聚簇索引获得行记录对应的值。
        * 优点：避免直接读取磁盘

InnoDb实现了四个标准的隔离级别，默认级别是可重复读（REPEATABLE READ）。

* 在可重复读隔离级别下，通过多版本并发控制（MVCC）+ Next-Key Locking 防止幻影读。。

**InnoDb内部做了很多优化:**

* 从磁盘读取数据时采用的可预测性读、
* 能够加快读操作并且自动创建的自适应哈希索引、
* 能够加速插入操作的插入缓冲区等。

**InnoDb支持真正的在线热备份。**

* 其它存储引擎不支持在线热备份，
* 要获取一致性视图需要停止对所有表的写入，
* 而在读写混合场景中，停止写入可能也意味着停止读取。

---

## MYISAM

设计简单，数据以紧密格式存储。对于只读数据，或者表比较小、可以容忍修复操作，则依然可以使用它。

提供了大量的特性，包括**压缩表**、**空间数据索引**等。

* **MyISAM并不支持事务以及行级锁，而且一个毫无疑问的缺陷是崩溃后无法安全恢复。**
* 不支持行级锁，只能对整张表加锁，读取时会对需要读到的所有表加共享锁，写入时则对表加排它锁。
* 但在表有读取操作的同时，也可以往表中插入新的记录，这被称为并发插入（CONCURRENT INSERT）。

可以手工或者自动执行检查和修复操作，但是和事务恢复以及崩溃恢复不同，可能导致一些数据丢失，而且修复操作是非常慢的。

如果指定了 DELAY_KEY_WRITE 选项，在每次修改执行完成时，不会立即将修改的索引数据写入磁盘，而是会写到内存中的键缓冲区，只有在清理键缓冲区或者关闭表的时候才会将对应的索引块写入磁盘。这种方式可以极大的提升写入性能，但是在数据库或者主机崩溃时会造成索引损坏，需要执行修复操作。

**对于只读的数据，或者表比较小，可以忍受修复操作，则依然可以使用MyISAM**

* **（但请不要默认使用MyISAM，而是应该默认使用InnoDB）**

## InnoDB和MyISAM比较

- **事务**： InnoDB 支持事务，MyISAM 不支持事务。这是 MySQL 将默认存储引擎从 MyISAM 变成InnoDB 的重要原因之一。
- **锁粒度**：**InnoDB 最小的锁粒度是行锁，MyISAM 最小的锁粒度是表锁**。一个更新语句会锁住整张表，导致其他查询和更新都会被阻塞，因此并发访问受限。这也是 MySQL 将默认存储引擎从MyISAM 变成 InnoDB 的重要原因之一。
- **外键**： InnoDB 支持外键，而 MyISAM 不支持。对一个包含外键的 InnoDB 表转为 MYISAM 会失败。
- **索引**：MyISAM主键索引是非聚簇索引，InnoDB主键索引是聚簇索引
- 使用场景：
    - MyISAM：如果有大量的SELECT，使用MyISAM
    - InnoDB：如果有大量的INSERT或UPDATE，使用InnoDB表

---

# 慢查询

1 通过 set global slow_query_log=1 开启慢查询日志，或者在my.cnf中直接配置

 2 set global long_query_time 设置慢查询时间

3 通过 **Mysqldumpslow** 工具找到 出现次数多或者时间长的sql语句

4 用explain查看这些语句的执行计划 

key字段没用到索引的话，如果没有索引就创建覆盖索引。如果like字段以%开头就无法用索引。

`字段很多的大表分为小表，常用的联合查询可以建立中间表。关联查询分为多个小查询

 https://blog.csdn.net/qq_35571554/article/details/82800463 

## SQL语句慢的原因

https://tech.meituan.com/2014/06/30/mysql-index.html

分两种情况讨论：

1. 大多数情况下很正常，偶尔很慢
   - 数据库在刷新脏页，redo log写满了需要同步到磁盘
   - 执行时遇到锁：行级锁、表级索
2. 这条SQL语句一直都很慢
   - 没用上索引：例如：该字段没有索引；由于对字段进行运算、函数操作导致无法用索引。
   - 数据库选错索引

3. 刷新脏页

当我们要往数据库插入一条数据、或者要更新一条数据的时候，我们知道数据库会在内存中把对应字段的数据更新了，但是更新之后，这些更新的字段并不会马上同步持久化到磁盘中去，而是把这些更新的记录写入到 redo log 日记中去，等到空闲的时候，在通过 redo log 里的日记把最新的数据同步到磁盘中去。

- **redolog写满了：**redo log 里的容量是有限的，如果数据库一直很忙，更新又很频繁，这个时候 redo log 很快就会被写满了，这个时候就没办法等到空闲的时候再把数据同步到磁盘的，只能暂停其他操作，全身心来把数据同步到磁盘中去的，而这个时候，**就会导致我们平时正常的SQL语句突然执行的很慢**，所以说，数据库在在同步数据到磁盘的时候，就有可能导致我们的SQL语句执行的很慢了。
- **内存不够用了：**如果一次查询较多的数据，恰好碰到所查数据页不在内存中时，需要申请内存，而此时恰好内存不足的时候就需要淘汰一部分内存数据页，如果是干净页，就直接释放，如果恰好是脏页就需要刷脏页。

2. 执行时遇到锁

我们所要执行的语句涉及到的表或行，别人正在用，加了锁。

我们可以用 **show processlist**这个命令来查看当前的状态

3. 没用到索引

4. 字段没有索引：若你的查询字段刚好没有索引，那么只能全表扫描
5. 字段有索引，但是没用到
   - 在字段的左边有运算 where c-1 =1000
   - 函数操作，查询时对字段进行了函数操作，导致没用到索引

6. 数据库选错索引

主键索引和非主键索引是有区别的，主键索引存放的值是**整行字段的数据**，而非主键索引上存放的值不是整行字段的数据，而且存放**主键字段的值**。

我们如果走 c 这个字段的索引的话，最后会查询到对应主键的值，然后，再根据主键的值走主键索引，查询到整行数据返回。

系统会判断是全表扫描更快还是非主键索引扫描更快。

系统可能会判断失误

我们可以强制系统走索引的方式查询

```sql
select * from t force index(a) where c < 100 and c < 100000;
```

5. 如何排查原因

数据库的问题往往是一个累积的过程，不断累加的糟糕的查询语句会逐渐增加系统负载

1. 查询慢查询数量

   访问数据库中slow_log表

   ```sql
   SELECT * FROM slow_log where start_time > '2019/05/19 00:00:00'; 
   ```

   就能查找出一天以来的慢查询数量了

2. 查看当前正在执行的查询

```sql
select * from information_schema.processlist 
```

正常运行的数据库，因为一条查询的执行速度很快，被我们的select抓到的info不是null的查询数量会很少。我们这样负荷很大的库一般也就只能查到几条。如果一次能查到info非空的查询有几十条，那么也可以认为系统出问题了。

3. 系统问题定位

因为当务之急是尽快恢复系统的正常运行，因此影响最直接的做法是在processlist的查询结果中，查看有多少哪些查询处于lock状态，或者已经执行了很长时间，把这些process用kill指令干掉。通过不停的杀死这些可能会引发系统阻塞的process，最终能够暂时让系统逐步恢复到正常状态，当然这只是权宜之计。

此外，最重要的当然是分析到底是哪些查询为什么会引发系统阻塞，我们还是使用慢查询来做分析。

慢查询表查询结果里面有几个比较重要的指标：

start_time 开始时间，要通过这个参数，配合系统出问题的时间，定位哪些查询是罪魁祸首。

query_time 查询时间

rows_sent 和 rows_examined发送的结果数以及查询扫过的行数，这两个值特别重要，特别是 rows_examined。基本上就能告诉我们，哪个查询是需要注意的“大”查询。

实际操作中，我们也是把有大量rows_examined的查询一个个拿出来分析，**添加索引，修改查询语句的编写，来彻底的解决问题。**

# 分库分表

**订单数大千万级，数据库怎么优化** 

**分库分表的依据是什么?有用什么工具吗？**

**对数据分库分表了吗？用什么算法进行分库分表。。。**

**设计一个视频评论表**

分库分表分为水平拆分和垂直拆分。

垂直分表是基于数据库中的"列"进行，某个表字段较多，可以新建一张扩展表，将不经常用或字段长度较大的字段拆分出去到扩展表中。解决业务系统层面的耦合，业务清晰。

水平分表根据表内数据内在的逻辑关系，将同一个表中的记录通过hash，或者数值范围拆分到多个结构相同的表中。

分库分表之前，先做读写分离、索引优化等等。当数据量达到单表的瓶颈时候，再考虑分库分表。

- 订单表有做拆分么，怎么拆的？(垂直拆分和水平拆分)

- 水平拆分后查询过程描述下

- 如果落到某个分片的数据很大怎么办？(按照某种规则，比如哈希取模、range，将单张表拆分为多张表)

- 哈希取模会有什么问题么？(有的，数据分布不均，扩容缩容相对复杂 )

- 分库分表后怎么解决读写压力？(一主多从、多主多从)

- 拆分后主键怎么保证惟一？(UUID、Snowflake算法)

  因为要是分成多个表之后，每个表都是从 1 开始累加，这样是不对的，我们需要一个全局唯一的 id 来支持。

  生成全局 id 有下面这几种方式：

  - **UUID**：不适合作为主键，因为太长了，并且无序不可读，查询效率低。比较适合用于生成唯一的名字的标示比如文件的名字。
  - **数据库自增 id** : 两台数据库分别设置不同步长，生成不重复ID的策略来实现高可用。这种方式生成的 id 有序，但是需要独立部署数据库实例，成本高，还会有性能瓶颈。
  - **利用 redis 生成 id :** 性能比较好，灵活方便，不依赖于数据库。但是，引入了新的组件造成系统更加复杂，可用性降低，编码更加复杂，增加了系统成本。
  - **Twitter的snowflake算法** ：
  - **美团的[Leaf](https://tech.meituan.com/2017/04/21/mt-leaf.html)分布式ID生成系统** ：Leaf 是美团开源的分布式ID生成器，能保证全局唯一性、趋势递增、单调递增、信息安全，里面也提到了几种分布式方案的对比，但也需要依赖关系数据库、Zookeeper等中间件。

- Snowflake生成的ID是全局递增唯一么？(不是，只是全局唯一，单机递增)

# 问题

### 数据库表设计

**你会怎么设计数据库表结构**

**数据库表设计**

分析业务，细化模块。

用户登录模块 ： 
step1： 用户表： 
必须字段—> 用户ID(主键)+用户名+密码 

选择合适的数据类型

- 使用可存下数据的最小的数据类型
- 使用not null定义字段

2.选择合适的索引列

- 不要滥用索引，遵循最左前缀原则
- （1）查询频繁的列，在where，group by，order by，on从句中出现的列
- （2）where条件中<，<=，=，>，>=，between，in，以及like 字符串+通配符（%）出现的列
- （3）长度小的列，索引字段越小越好，因为数据库的存储单位是页，一页中能存下的数据越多越好
- （4）离散度大（不同的值多）的列，放在联合索引前面。查看离散度，通过统计不同的列值来实现，count越大，离散程度越高：

### **优化**

项目中你对数据库做了什么优化？

数据库优化

查询where a>5 and b>10的时候，数据库做了哪些优化

分库分表，读写分离，sql语句优化

A不用自带的函数

B：连续数值条件，用BETWEEN不用IN：SELECT id FROM t WHERE num BETWEEN 1 AND 5
C：不建议使用 select * from t ，用具体的字段列表代替“
D：where后的条件要符合最左前缀原则

### **引擎**

Mysql用过那个引擎

mysql数据库引擎及其区别

时序数据库（不知道） 

mysql中分页查询？

事务，分布式事务

### **锁**

**MySQL锁机制**

不同的存储引擎支持不同的锁机制	

**MySQL行锁和表锁有用过吗**

用到索引就是行锁，没用索引表锁

**知道MySQL插入和查询分别用的是什么锁吗**

读锁，写锁

**知道悲观锁吗？了解多少？**

synchronize lock 排他锁

**myql间歇锁的实现原理**

Java和mysql的锁介绍，乐观锁和悲观锁

### **索引**

**项目中你是根据什么来进行索引的设计的？**

1：最适合索引的列是出现在WHERE⼦句中的列，利⽤最左前缀，（A,B,C)走A,B

2：使⽤唯⼀索引。考虑某列中值的分布。索引的列的基数越⼤，索引的效果越好。对出生日期做索引而不是性别

3：使⽤短索引。如果对字符串列进⾏索引，应该指定⼀个前缀长度，只要有可能就应该这样做。例如，有⼀个 CHAR(200)列，如果在前10个或20个字符内，多数值是唯⼀的，那么就不要对整个列进⾏索引。对前10个或20个字 符进⾏索引能够节省⼤量索引空间。

4：不要过度索引。索引太多，也 可能会使MySQL选择不到所要使⽤的最好索引。只保持所需的索引有利于查询优化。
原文链接：https://blog.csdn.net/qq_35400008/article/details/81563609

**数据库有哪些索引？**

b+树索引，hash索引，全文索引

数据库中不同索引的区别

数据库多个索引，底层原理

说一下索引的原理

插入一个数据对索引的影响，讲下过程

MySQL索引，插入一个数据对索引的影响（本质是B+树的插入）

根据key值找到叶子结点，向这个叶子结点插入记录。插入后，若当前结点key的个数大于m-1，叶子结点分裂成左右两个叶子结点，左叶子结点包含前m/2个记录，右结点包含剩下的记录，将第m/2+1个记录的key进位到父结点中（父结点一定是索引类型结点），进位到父结点的key左孩子指针向左结点,右孩子指针向右结点。

MySQL建立索引后，有什么不同（查询效率，全表扫描和B+树的查询）

如何防止sql注入

### **事务，隔离级别** 

数据库隔离级别、

隔离级别 mysql数据库引擎及其区别，不同数据库索引 （hash以及B+）的应用场景。

分布式事务

MySQL和NoSQL的应用场景

### **主从**

mysql如何置master-slave？

​    mysql里面的日志讲一下，binlog redolog和undolog    

​    建一张表，如何创建索引，你是怎么去考虑的    

​    数据库的分库分表有了解过吗？有哪些实现方式。    

​    Redis的事务和mysql的事务有哪些区别。

---



### 数据库高并发场景下怎么应对？

https://juejin.cn/post/6941268421073960968

https://segmentfault.com/a/1190000021562093



1. 数据库字段优化，索引优化
    1. 尽可能多的保证命中索引
2. 优化sql语句
    1. 优化掉执行慢点一些语句
3. 加缓存，redis
4. 通过消息队列削峰异步处理
5. 延迟处理请求
6. 前端随机打散请求
7. 主从复制，读写分离
8. 分库分表
    1. 将一个库拆分为多个库
    2. 将一张表拆分为多个表
9. 

----

### 数据库在高并发场景下怎么安全应对？

场景：当有多个线程尝试修改一个账户余额时，应该怎么保证不会对一个账户重复扣费？



